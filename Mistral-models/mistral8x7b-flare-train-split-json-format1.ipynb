{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "from huggingface_hub import login\n",
    "login()## Let's begin!\n",
    "Notebook based on brevdev notebook: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade8574bb07741fe8f920a681e05d41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='../data_files/json_format1/5w1h_subtask_1_zero_train_train_json_format1.json', encoding = 'utf-8',split='train')\n",
    "eval_dataset = load_dataset('json', data_files='../data_files/json_format1/5w1h_subtask_1_zero_train_test_json_format1.json', encoding = 'utf-8', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\\n\\nWHO: Sujetos o entidades involucradas.\\nWHAT: Hechos u objetos mencionados.\\nWHEN: Detalles relacionados con el tiempo.\\nWHERE: Lugares mencionados.\\nWHY: Causas o razones.\\nHOW: Maneras o métodos descritos.\\n\\nAbajo es un ejemplo:\\n\\nInput: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\\n\\nAhora, completa la siguiente tarea:\\n\\nInput: La decisión, señala el dictamen, corresponde a la persona afectada, que puede prestar o negar su \\xa0consentimiento.\",\n",
       " 'tags': [{'5W1H_Label': 'WHAT',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 11,\n",
       "   'Tag_Start': 0,\n",
       "   'Tag_Text': 'La decisión'},\n",
       "  {'5W1H_Label': 'WHO',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 66,\n",
       "   'Tag_Start': 45,\n",
       "   'Tag_Text': 'a la persona afectada'},\n",
       "  {'5W1H_Label': 'WHAT',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 112,\n",
       "   'Tag_Start': 94,\n",
       "   'Tag_Text': 'su \\xa0consentimiento'}],\n",
       " 'Id': 345,\n",
       " 'output': {'HOW': None,\n",
       "  'WHAT': ['La decisión', 'su \\xa0consentimiento'],\n",
       "  'WHEN': None,\n",
       "  'WHERE': None,\n",
       "  'WHO': ['a la persona afectada'],\n",
       "  'WHY': None}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator ([description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"Flares-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"{example['input']} Output: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"{example['input']} Output: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JupyterLab v3.4.6\n",
      "/opt/conda/share/jupyter/labextensions\n",
      "        jupyterlab_pygments v0.2.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_pygments)\n",
      "        jupyter-matplotlib v0.11.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m\n",
      "        @jupyter-widgets/jupyterlab-manager v5.0.3 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_widgets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc678e525b534723a5d849f9f782e2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'mixtral-8x7b-v0.1.Q4_K_M.gguf' to '/home/jovyan/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-v0.1-GGUF/blobs/5e066c60d89d904db46c3bf577661040578a223a5a39ba3fd23ff091549767f0.incomplete' (resume from 2726297600/26441532864)\n",
      "mixtral-8x7b-v0.1.Q4_K_M.gguf: 100%|███████| 26.4G/26.4G [33:20<00:00, 11.9MB/s]\n",
      "Download complete. Moving file to /home/jovyan/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-v0.1-GGUF/blobs/5e066c60d89d904db46c3bf577661040578a223a5a39ba3fd23ff091549767f0\n",
      "/home/jovyan/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-v0.1-GGUF/snapshots/38762deaceb8f976428ab44e4c3ccf927a91132a/mixtral-8x7b-v0.1.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/Mixtral-8x7B-v0.1-GGUF mixtral-8x7b-v0.1.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIhCAYAAABE54vcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKcklEQVR4nO3deVxVdf7H8feVHYSrgnAhUVFxSdw1S53QECsVS5vRMtd0xsYyKU1r2qgpSCqXcrJsGrWsbBlxtMUR18nUQo1Sc9QxdyFaiEUJFM7vjx6cX1dAOYhewNfz8biPR/d7vvecz7nfC/H2e8732gzDMAQAAAAAqLR6ri4AAAAAAGobghQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUgDpj8eLFstls5sPb21sOh0P9+vVTUlKSsrKyyrwmISFBNpvN0nFOnz6thIQEbdy40dLryjtW8+bNNXjwYEv7uZC3335bc+fOLXebzWZTQkJCtR6vuq1bt07du3eXn5+fbDabVqxYUW6/w4cPy2az6fnnn7+8BVqQmJhYbv2ln9Xt27df/qLK8eijj6pp06Zyd3dXgwYNKuxXlZ+XS+nkyZNKSEhQenq65deWfn4WL158wb417bwB1AwEKQB1zqJFi7R161alpqbqb3/7mzp37qxZs2apXbt2Wrt2rVPfiRMnauvWrZb2f/r0aT355JOWg1RVjlUV5wtSW7du1cSJEy95DVVlGIaGDx8uDw8PrVy5Ulu3blV0dLSry6qyioJUTfKvf/1LzzzzjMaMGaNNmzaV+Rn5rcv1Ga6skydP6sknn6xSkAoNDdXWrVs1aNCg6i8MwBXB3dUFAEB1i4qKUvfu3c3nt912m+6//3716dNHw4YN04EDBxQSEiJJatKkiZo0aXJJ6zl9+rR8fX0vy7Eu5Nprr3Xp8S/k5MmT+umnnzR06FDFxMS4upwrwu7duyVJ9913n4KDg8/btyZ8hquLl5dXjf95AFCzMSMF4IrQtGlTvfDCC8rLy9Orr75qtpd3yc769evVt29fBQYGysfHR02bNtVtt92m06dP6/Dhw2rcuLEk6cknnzQvIxw3bpzT/nbu3Knf//73atiwoVq2bFnhsUqlpKSoY8eO8vb2VosWLfTiiy86bS+9FOzw4cNO7Rs3bpTNZjNnx/r27auPPvpIR44ccbrMsVR5l/bt3r1bt9xyixo2bChvb2917txZS5YsKfc477zzjh555BGFhYUpICBA/fv31759+yp+439j8+bNiomJkb+/v3x9fdWrVy999NFH5vaEhATzj/SZM2fKZrOpefPmldr3+eTm5mr69OmKiIiQp6enrrrqKsXHx+vUqVNO/Ww2m+699169+eabateunXx9fdWpUyd9+OGHZfb5r3/9Sx07dpSXl5datGihefPmlRlfm82mU6dOacmSJeY49O3b12k/eXl5+vOf/6ygoCAFBgZq2LBhOnnypFOf830ez6ekpETJyclq27atvLy8FBwcrDFjxuj48eNmn+bNm+vRRx+VJIWEhFzw0s/zXZ66evVqde3aVT4+Pmrbtq3+8Y9/OPUr/QynpqZq/PjxatSokfz8/BQXF6dvv/22zD5Lf6Z+q2/fvuZ7uHHjRvXo0UOSNH78ePM9ruylqxVd2vfRRx+pc+fO8vLyUkRERIWXjr7//vvq2bOn7Ha7fH191aJFC911112VOjaAuoEZKQBXjIEDB8rNzU3/+c9/Kuxz+PBhDRo0SL/73e/0j3/8Qw0aNNCJEye0evVqFRUVKTQ0VKtXr9ZNN92kCRMmmJfJlYarUsOGDdPtt9+uu+++u8wf7OdKT09XfHy8EhIS5HA49NZbb2nq1KkqKirS9OnTLZ3jyy+/rD/96U86ePCgUlJSLth/37596tWrl4KDg/Xiiy8qMDBQS5cu1bhx4/Tdd99pxowZTv3/8pe/qHfv3vr73/+u3NxczZw5U3Fxcdq7d6/c3NwqPM6mTZsUGxurjh076vXXX5eXl5defvllxcXF6Z133tGIESM0ceJEderUScOGDdOUKVM0cuRIeXl5WTr/c50+fVrR0dE6fvy4/vKXv6hjx47as2ePHn/8ce3atUtr1651CgYfffSR0tLS9NRTT6l+/fpKTk7W0KFDtW/fPrVo0UKStHr1ag0bNkzXX3+93n33XZ09e1bPP/+8vvvuO6djb926VTfccIP69eunxx57TJIUEBDg1GfixIkaNGiQ3n77bR07dkwPPvigRo0apfXr10u68OfR19e3wnP/85//rIULF+ree+/V4MGDdfjwYT322GPauHGjdu7cqaCgIKWkpOhvf/ubXn/9da1evVp2u71KM05fffWVpk2bpoceekghISH6+9//rgkTJqhVq1a6/vrrnfpOmDBBsbGx5jk/+uij6tu3r77++uvz3p91rq5du2rRokUaP368Hn30UfMSvYuZMVu3bp1uueUWXXfddVq2bJmKi4uVnJxc7tiOGDFCI0aMUEJCgry9vXXkyBFz3ABcIQwAqCMWLVpkSDLS0tIq7BMSEmK0a9fOfP7EE08Yv/1V+MEHHxiSjPT09Ar38f333xuSjCeeeKLMttL9Pf744xVu+61mzZoZNputzPFiY2ONgIAA49SpU07ndujQIad+GzZsMCQZGzZsMNsGDRpkNGvWrNzaz6379ttvN7y8vIyjR4869bv55psNX19f4+eff3Y6zsCBA536vffee4YkY+vWreUer9S1115rBAcHG3l5eWbb2bNnjaioKKNJkyZGSUmJYRiGcejQIUOS8dxzz513f5Xtm5SUZNSrV6/MZ6J0nD/++GOzTZIREhJi5Obmmm2ZmZlGvXr1jKSkJLOtR48eRnh4uFFYWGi25eXlGYGBgWXG18/Pzxg7dmyZukrHc/LkyU7tycnJhiQjIyPDqc7zfR7Ls3fv3nL3//nnnxuSjL/85S9mW+nn8vvvv7/gfiv6DHt7extHjhwx2woKCoxGjRoZkyZNMttKz3no0KFOr//ss88MScbTTz/ttM/y3rfo6GgjOjrafJ6WlmZIMhYtWnTB2s9V+vn57Wt79uxphIWFGQUFBWZbbm6u0ahRI6fzfv755w1J5s8HgCsTl/YBuKIYhnHe7Z07d5anp6f+9Kc/acmSJWUuOaqs2267rdJ927dvr06dOjm1jRw5Urm5udq5c2eVjl9Z69evV0xMjMLDw53ax40bp9OnT5dZWGDIkCFOzzt27ChJOnLkSIXHOHXqlD7//HP9/ve/V/369c12Nzc3jR49WsePH6/05YFWffjhh4qKilLnzp119uxZ83HjjTc6XRJZql+/fvL39zefh4SEKDg42Dy/U6dOafv27br11lvl6elp9qtfv77i4uIs13eh97Oqn8cNGzZIUpnL46655hq1a9dO69ats1zr+XTu3FlNmzY1n3t7e6t169blfi7uvPNOp+e9evVSs2bNzJpd5dSpU0pLS9OwYcPk7e1ttvv7+5cZ29JLCocPH6733ntPJ06cuKy1AqgZCFIArhinTp3Sjz/+qLCwsAr7tGzZUmvXrlVwcLDuuecetWzZUi1bttS8efMsHSs0NLTSfR0OR4VtP/74o6XjWvXjjz+WW2vpe3Tu8QMDA52el156V1BQUOExsrOzZRiGpeNUl++++05ff/21PDw8nB7+/v4yDEM//PCDU/9zz0/69RxLz6/0XEoXK/mt8tou5ELvZ1U/j6XvZ0XveXW/3xd6336ros/7pf6sX0h2drZKSkrO+/NY6vrrr9eKFSt09uxZjRkzRk2aNFFUVJTeeeedy1UugBqAe6QAXDE++ugjFRcXl7nh/1y/+93v9Lvf/U7FxcXavn27XnrpJcXHxyskJES33357pY5l5TtnMjMzK2wr/QO19F/ICwsLnfqdGwSsCgwMVEZGRpn20gUPgoKCLmr/ktSwYUPVq1fvkh+nPEFBQfLx8Smz8MFvt1vRsGFD2Wy2MvfMSOWPY3Woyuex9HOTkZFR5p6hkydPXrL3uzIq+ry3atXKfO7t7V3msy79+nm/VLWXju35fh5/65ZbbtEtt9yiwsJCbdu2TUlJSRo5cqSaN2+u66677pLUCKBmYUYKwBXh6NGjmj59uux2uyZNmlSp17i5ualnz57629/+JknmZXaVmYWxYs+ePfrqq6+c2t5++235+/ura9eukmSuXvf111879Vu5cmWZ/VU0E1CemJgYrV+/vsxKcW+88YZ8fX2rZXloPz8/9ezZU8uXL3eqq6SkREuXLlWTJk3UunXriz5OeQYPHqyDBw8qMDBQ3bt3L/Owuiqgn5+funfvrhUrVqioqMhsz8/PL3d1PytjcSEVfR7Lc8MNN0iSli5d6tSelpamvXv3unRp+bfeesvp+ZYtW3TkyBGnf+Bo3rx5mc/6/v37y1wCWp0/i35+frrmmmu0fPly/fLLL2Z7Xl6eVq1aVeHrvLy8FB0drVmzZkmSvvzyy4uuBUDtwIwUgDpn9+7d5r0wWVlZ+vTTT7Vo0SK5ubkpJSWlzAp7v/XKK69o/fr1GjRokJo2bapffvnFnM3o37+/pF/vmWjWrJn+9a9/KSYmRo0aNVJQUFCVl+oOCwvTkCFDlJCQoNDQUC1dulSpqamaNWuWuSpbjx491KZNG02fPl1nz55Vw4YNlZKSos2bN5fZX4cOHbR8+XItWLBA3bp1U7169Zy+V+u3nnjiCX344Yfq16+fHn/8cTVq1EhvvfWWPvroIyUnJ8tut1fpnM6VlJSk2NhY9evXT9OnT5enp6defvll7d69W++8846lGbxz7dq1Sx988EGZ9h49eig+Pl7//Oc/df311+v+++9Xx44dVVJSoqNHj2rNmjWaNm2aevbsael4Tz31lAYNGqQbb7xRU6dOVXFxsZ577jnVr19fP/30k1PfDh06aOPGjVq1apVCQ0Pl7++vNm3aVPpYlfk8lqdNmzb605/+pJdeekn16tXTzTffbK7aFx4ervvvv9/SOVen7du3a+LEifrDH/6gY8eO6ZFHHtFVV12lyZMnm31Gjx6tUaNGafLkybrtttt05MgRJScnl/nZbdmypXx8fPTWW2+pXbt2ql+/vsLCws57+e75/PWvf9VNN92k2NhYTZs2TcXFxZo1a5b8/Pycxvbxxx/X8ePHFRMToyZNmujnn3/WvHnz5OHhUau/QBqARa5d6wIAqk/pqmClD09PTyM4ONiIjo42EhMTjaysrDKvOXcVsq1btxpDhw41mjVrZnh5eRmBgYFGdHS0sXLlSqfXrV271ujSpYvh5eVlSDJXGDvfCmgVrXg2aNAg44MPPjDat29veHp6Gs2bNzdmz55d5vX79+83BgwYYAQEBBiNGzc2pkyZYnz00UdlVu376aefjN///vdGgwYNDJvN5nRMlbPa4K5du4y4uDjDbrcbnp6eRqdOncqsgla6at/777/v1F7eymcV+fTTT40bbrjB8PPzM3x8fIxrr73WWLVqVbn7s7JqX0WP0pry8/ONRx991GjTpo3h6elp2O12o0OHDsb9999vZGZmOr0399xzT5njlLeCXEpKitGhQwfD09PTaNq0qfHss88a9913n9GwYUOnfunp6Ubv3r0NX19fQ5K54lxFK0yeuwpjZT+P5SkuLjZmzZpltG7d2vDw8DCCgoKMUaNGGceOHXPqVx2r9g0aNKhM33NX2Cs95zVr1hijR482GjRoYPj4+BgDBw40Dhw44PTakpISIzk52WjRooXh7e1tdO/e3Vi/fn2ZfRqGYbzzzjtG27ZtDQ8PjwpX0yxPRZ/dlStXGh07dnQa23PP+8MPPzRuvvlm46qrrjJ/zwwcOND49NNPK3VsAHWDzTAusIQVAAA4rzNnzqhz58666qqrtGbNGleXUyMtXrxY48ePV1paWoUzpABQm3BpHwAAFpV+qWxoaKgyMzP1yiuvaO/evZZXdwQA1F4EKQAALMrLy9P06dP1/fffy8PDQ127dtXHH3983vuWcHkYhqHi4uLz9nFzc7uo+/IAQJK4tA8AANQZGzduVL9+/c7bZ9GiRWW+rBgArCJIAQCAOiMvL6/MMunnioiIKPdLhAHACoIUAAAAAFjEF/ICAAAAgEUsNiGppKREJ0+elL+/PzefAgAAAFcwwzCUl5ensLAw1atX8bwTQUrSyZMnFR4e7uoyAAAAANQQx44dU5MmTSrcTpCS5O/vL+nXNysgIMDF1QAAAABwldzcXIWHh5sZoSIEKcm8nC8gIIAgBQAAAOCCt/yw2AQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgkburC0DNFhfn6gqcrVrl6goAAAAAZqQAAAAAwDKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABa5u7oAlBUX5+oKAAAAAJyPy2ekTpw4oVGjRikwMFC+vr7q3LmzduzYYW43DEMJCQkKCwuTj4+P+vbtqz179jjto7CwUFOmTFFQUJD8/Pw0ZMgQHT9+/HKfCgAAAIArhEuDVHZ2tnr37i0PDw998skn+uabb/TCCy+oQYMGZp/k5GTNnj1b8+fPV1pamhwOh2JjY5WXl2f2iY+PV0pKipYtW6bNmzcrPz9fgwcPVnFxsQvOCgAAAEBdZzMMw3DVwR966CF99tln+vTTT8vdbhiGwsLCFB8fr5kzZ0r6dfYpJCREs2bN0qRJk5STk6PGjRvrzTff1IgRIyRJJ0+eVHh4uD7++GPdeOONF6wjNzdXdrtdOTk5CggIqL4TrCIu7avYqlWurgAAAAB1WWWzgUtnpFauXKnu3bvrD3/4g4KDg9WlSxe99tpr5vZDhw4pMzNTAwYMMNu8vLwUHR2tLVu2SJJ27NihM2fOOPUJCwtTVFSU2edchYWFys3NdXoAAAAAQGW5NEh9++23WrBggSIjI/Xvf/9bd999t+677z698cYbkqTMzExJUkhIiNPrQkJCzG2ZmZny9PRUw4YNK+xzrqSkJNntdvMRHh5e3acGAAAAoA5zaZAqKSlR165dlZiYqC5dumjSpEn64x//qAULFjj1s9lsTs8NwyjTdq7z9Xn44YeVk5NjPo4dO3ZxJwIAAADgiuLSIBUaGqqrr77aqa1du3Y6evSoJMnhcEhSmZmlrKwsc5bK4XCoqKhI2dnZFfY5l5eXlwICApweAAAAAFBZLg1SvXv31r59+5za9u/fr2bNmkmSIiIi5HA4lJqaam4vKirSpk2b1KtXL0lSt27d5OHh4dQnIyNDu3fvNvsAAAAAQHVy6Rfy3n///erVq5cSExM1fPhwffHFF1q4cKEWLlwo6ddL+uLj45WYmKjIyEhFRkYqMTFRvr6+GjlypCTJbrdrwoQJmjZtmgIDA9WoUSNNnz5dHTp0UP/+/V15egAAAADqKJcGqR49eiglJUUPP/ywnnrqKUVERGju3Lm68847zT4zZsxQQUGBJk+erOzsbPXs2VNr1qyRv7+/2WfOnDlyd3fX8OHDVVBQoJiYGC1evFhubm6uOC0AAAAAdZxLv0eqpuB7pGoPvkcKAAAAl1Kt+B4pAAAAAKiNCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALHJpkEpISJDNZnN6OBwOc7thGEpISFBYWJh8fHzUt29f7dmzx2kfhYWFmjJlioKCguTn56chQ4bo+PHjl/tUAAAAAFxBXD4j1b59e2VkZJiPXbt2mduSk5M1e/ZszZ8/X2lpaXI4HIqNjVVeXp7ZJz4+XikpKVq2bJk2b96s/Px8DR48WMXFxa44HQAAAABXAHeXF+Du7jQLVcowDM2dO1ePPPKIhg0bJklasmSJQkJC9Pbbb2vSpEnKycnR66+/rjfffFP9+/eXJC1dulTh4eFau3atbrzxxnKPWVhYqMLCQvN5bm7uJTgzAAAAAHWVy2ekDhw4oLCwMEVEROj222/Xt99+K0k6dOiQMjMzNWDAALOvl5eXoqOjtWXLFknSjh07dObMGac+YWFhioqKMvuUJykpSXa73XyEh4dforMDAAAAUBe5NEj17NlTb7zxhv7973/rtddeU2Zmpnr16qUff/xRmZmZkqSQkBCn14SEhJjbMjMz5enpqYYNG1bYpzwPP/ywcnJyzMexY8eq+cwAAAAA1GUuvbTv5ptvNv+7Q4cOuu6669SyZUstWbJE1157rSTJZrM5vcYwjDJt57pQHy8vL3l5eV1E5QAAAACuZC6/tO+3/Pz81KFDBx04cMC8b+rcmaWsrCxzlsrhcKioqEjZ2dkV9gEAAACA6lajglRhYaH27t2r0NBQRUREyOFwKDU11dxeVFSkTZs2qVevXpKkbt26ycPDw6lPRkaGdu/ebfYBAAAAgOrm0kv7pk+frri4ODVt2lRZWVl6+umnlZubq7Fjx8pmsyk+Pl6JiYmKjIxUZGSkEhMT5evrq5EjR0qS7Ha7JkyYoGnTpikwMFCNGjXS9OnT1aFDB3MVPwAAAACobi4NUsePH9cdd9yhH374QY0bN9a1116rbdu2qVmzZpKkGTNmqKCgQJMnT1Z2drZ69uypNWvWyN/f39zHnDlz5O7uruHDh6ugoEAxMTFavHix3NzcXHVaAAAAAOo4m2EYhquLcLXc3FzZ7Xbl5OQoICDA1eUoLs7VFdRcq1a5ugIAAADUZZXNBjXqHikAAAAAqA0IUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYFGNCVJJSUmy2WyKj4832wzDUEJCgsLCwuTj46O+fftqz549Tq8rLCzUlClTFBQUJD8/Pw0ZMkTHjx+/zNUDAAAAuJLUiCCVlpamhQsXqmPHjk7tycnJmj17tubPn6+0tDQ5HA7FxsYqLy/P7BMfH6+UlBQtW7ZMmzdvVn5+vgYPHqzi4uLLfRoAAAAArhAuD1L5+fm688479dprr6lhw4Zmu2EYmjt3rh555BENGzZMUVFRWrJkiU6fPq23335bkpSTk6PXX39dL7zwgvr3768uXbpo6dKl2rVrl9auXeuqUwIAAABQx7k8SN1zzz0aNGiQ+vfv79R+6NAhZWZmasCAAWabl5eXoqOjtWXLFknSjh07dObMGac+YWFhioqKMvuUp7CwULm5uU4PAAAAAKgsd1cefNmyZdq5c6fS0tLKbMvMzJQkhYSEOLWHhIToyJEjZh9PT0+nmazSPqWvL09SUpKefPLJiy0fAAAAwBXKZTNSx44d09SpU7V06VJ5e3tX2M9mszk9NwyjTNu5LtTn4YcfVk5Ojvk4duyYteIBAAAAXNFcFqR27NihrKwsdevWTe7u7nJ3d9emTZv04osvyt3d3ZyJOndmKSsry9zmcDhUVFSk7OzsCvuUx8vLSwEBAU4PAAAAAKgslwWpmJgY7dq1S+np6eaje/fuuvPOO5Wenq4WLVrI4XAoNTXVfE1RUZE2bdqkXr16SZK6desmDw8Ppz4ZGRnavXu32QcAAAAAqpvL7pHy9/dXVFSUU5ufn58CAwPN9vj4eCUmJioyMlKRkZFKTEyUr6+vRo4cKUmy2+2aMGGCpk2bpsDAQDVq1EjTp09Xhw4dyixeAQAAAADVxaWLTVzIjBkzVFBQoMmTJys7O1s9e/bUmjVr5O/vb/aZM2eO3N3dNXz4cBUUFCgmJkaLFy+Wm5ubCysHAAAAUJfZDMMwXF2Eq+Xm5sputysnJ6dG3C8VF+fqCmquVatcXQEAAADqsspmA5d/jxQAAAAA1DYEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALKpSkDp06FB11wEAAAAAtUaVglSrVq3Ur18/LV26VL/88kt11wQAAAAANVqVgtRXX32lLl26aNq0aXI4HJo0aZK++OKL6q4NAAAAAGqkKgWpqKgozZ49WydOnNCiRYuUmZmpPn36qH379po9e7a+//776q4TAAAAAGqMi1pswt3dXUOHDtV7772nWbNm6eDBg5o+fbqaNGmiMWPGKCMjo7rqBAAAAIAa46KC1Pbt2zV58mSFhoZq9uzZmj59ug4ePKj169frxIkTuuWWW6qrTgAAAACoMdyr8qLZs2dr0aJF2rdvnwYOHKg33nhDAwcOVL16v+ayiIgIvfrqq2rbtm21FgsAAAAANUGVgtSCBQt01113afz48XI4HOX2adq0qV5//fWLKg4AAAAAaqIqBakDBw5csI+np6fGjh1bld0DAAAAQI1WpXukFi1apPfff79M+/vvv68lS5ZcdFEAAAAAUJNVKUg9++yzCgoKKtMeHBysxMTEiy4KAAAAAGqyKgWpI0eOKCIiokx7s2bNdPTo0YsuCgAAAABqsioFqeDgYH399ddl2r/66isFBgZedFEAAAAAUJNVKUjdfvvtuu+++7RhwwYVFxeruLhY69ev19SpU3X77bdXd40AAAAAUKNUadW+p59+WkeOHFFMTIzc3X/dRUlJicaMGcM9UgAAAADqvCoFKU9PT7377rv661//qq+++ko+Pj7q0KGDmjVrVt31AQAAAECNU6UgVap169Zq3bp1ddUCAAAAALVClYJUcXGxFi9erHXr1ikrK0slJSVO29evX18txQEAAABATVSlIDV16lQtXrxYgwYNUlRUlGw2W3XXBQAAAAA1VpWC1LJly/Tee+9p4MCB1V0PAAAAANR4VVr+3NPTU61ataruWgAAAACgVqhSkJo2bZrmzZsnwzCqux4AAAAAqPGqdGnf5s2btWHDBn3yySdq3769PDw8nLYvX768WooDAAAAgJqoSkGqQYMGGjp0aHXXAgAAAAC1QpWC1KJFi6q7DgAAAACoNap0j5QknT17VmvXrtWrr76qvLw8SdLJkyeVn59fbcUBAAAAQE1UpRmpI0eO6KabbtLRo0dVWFio2NhY+fv7Kzk5Wb/88oteeeWV6q4TAAAAAGqMKs1ITZ06Vd27d1d2drZ8fHzM9qFDh2rdunXVVhwAAAAA1ERVXrXvs88+k6enp1N7s2bNdOLEiWopDAAAAABqqirNSJWUlKi4uLhM+/Hjx+Xv73/RRQEAAABATValIBUbG6u5c+eaz202m/Lz8/XEE09o4MCB1VUbAAAAANRIVbq0b86cOerXr5+uvvpq/fLLLxo5cqQOHDigoKAgvfPOO9VdIwAAAADUKFUKUmFhYUpPT9c777yjnTt3qqSkRBMmTNCdd97ptPgEAAAAANRFVQpSkuTj46O77rpLd911V3XWAwAAAAA1XpWC1BtvvHHe7WPGjKlSMQAAAABQG1QpSE2dOtXp+ZkzZ3T69Gl5enrK19eXIAUAAACgTqvSqn3Z2dlOj/z8fO3bt099+vRhsQkAAAAAdV6VglR5IiMj9eyzz5aZrQIAAACAuqbagpQkubm56eTJk9W5SwAAAACocap0j9TKlSudnhuGoYyMDM2fP1+9e/eulsIAAAAAoKaqUpC69dZbnZ7bbDY1btxYN9xwg1544YXqqAsAAAAAaqwqBamSkpLqrgMAAAAAao1qvUcKAAAAAK4EVZqReuCBByrdd/bs2VU5BAAAAADUWFUKUl9++aV27typs2fPqk2bNpKk/fv3y83NTV27djX72Wy26qkSAAAAAGqQKgWpuLg4+fv7a8mSJWrYsKGkX7+kd/z48frd736nadOmVWuRAAAAAFCT2AzDMKy+6KqrrtKaNWvUvn17p/bdu3drwIABte67pHJzc2W325WTk6OAgABXl6O4OFdXUHOtWuXqCgAAAFCXVTYbVGmxidzcXH333Xdl2rOyspSXl1eVXQIAAABArVGlIDV06FCNHz9eH3zwgY4fP67jx4/rgw8+0IQJEzRs2LDqrhEAAAAAapQq3SP1yiuvaPr06Ro1apTOnDnz647c3TVhwgQ999xz1VogAAAAANQ0VbpHqtSpU6d08OBBGYahVq1ayc/Przpru2y4R6r24B4pAAAAXEqX9B6pUhkZGcrIyFDr1q3l5+cnq5lswYIF6tixowICAhQQEKDrrrtOn3zyibndMAwlJCQoLCxMPj4+6tu3r/bs2eO0j8LCQk2ZMkVBQUHy8/PTkCFDdPz48Ys5LQAAAAA4ryoFqR9//FExMTFq3bq1Bg4cqIyMDEnSxIkTLS193qRJEz377LPavn27tm/frhtuuEG33HKLGZaSk5M1e/ZszZ8/X2lpaXI4HIqNjXVa0CI+Pl4pKSlatmyZNm/erPz8fA0ePFjFxcVVOTUAAAAAuKAqBan7779fHh4eOnr0qHx9fc32ESNGaPXq1ZXeT1xcnAYOHKjWrVurdevWeuaZZ1S/fn1t27ZNhmFo7ty5euSRRzRs2DBFRUVpyZIlOn36tN5++21JUk5Ojl5//XW98MIL6t+/v7p06aKlS5dq165dWrt2bVVODQAAAAAuqEpBas2aNZo1a5aaNGni1B4ZGakjR45UqZDi4mItW7ZMp06d0nXXXadDhw4pMzNTAwYMMPt4eXkpOjpaW7ZskSTt2LFDZ86cceoTFhamqKgos095CgsLlZub6/QAAAAAgMqqUpA6deqU00xUqR9++EFeXl6W9rVr1y7Vr19fXl5euvvuu5WSkqKrr75amZmZkqSQkBCn/iEhIea2zMxMeXp6qmHDhhX2KU9SUpLsdrv5CA8Pt1QzAAAAgCtblYLU9ddfrzfeeMN8brPZVFJSoueee079+vWztK82bdooPT1d27Zt05///GeNHTtW33zzjdO+f8swjDJt57pQn4cfflg5OTnm49ixY5ZqBgAAAHBlq9L3SD333HPq27evtm/frqKiIs2YMUN79uzRTz/9pM8++8zSvjw9PdWqVStJUvfu3ZWWlqZ58+Zp5syZkn6ddQoNDTX7Z2VlmbNUDodDRUVFys7OdpqVysrKUq9evSo8ppeXl+WZMwAAAAAoVaUZqauvvlpff/21rrnmGsXGxurUqVMaNmyYvvzyS7Vs2fKiCjIMQ4WFhYqIiJDD4VBqaqq5raioSJs2bTJDUrdu3eTh4eHUJyMjQ7t37z5vkAIAAACAi2F5Rqp0cYdXX31VTz755EUd/C9/+YtuvvlmhYeHKy8vT8uWLdPGjRu1evVq2Ww2xcfHKzExUZGRkYqMjFRiYqJ8fX01cuRISZLdbteECRM0bdo0BQYGqlGjRpo+fbo6dOig/v37X1RtAAAAAFARy0HKw8NDu3fvvuB9SpXx3XffafTo0crIyJDdblfHjh21evVqxcbGSpJmzJihgoICTZ48WdnZ2erZs6fWrFkjf39/cx9z5syRu7u7hg8froKCAsXExGjx4sVyc3O76PoAAAAAoDw2wzAMqy+aNm2aPDw89Oyzz16Kmi673Nxc2e125eTkKCAgwNXlKC7O1RXUXKtWuboCAAAA1GWVzQZVWmyiqKhIf//735Wamqru3bvLz8/Pafvs2bOrslsAAAAAqBUsBalvv/1WzZs31+7du9W1a1dJ0v79+536VMclfwAAAABQk1kKUpGRkcrIyNCGDRskSSNGjNCLL75Y5ktzAQAAAKAus7T8+bm3U33yySc6depUtRYEAAAAADVdlb5HqlQV1qkAAAAAgFrPUpCy2Wxl7oHinigAAAAAVxpL90gZhqFx48bJy8tLkvTLL7/o7rvvLrNq3/Lly6uvQgAAAACoYSwFqbFjxzo9HzVqVLUWAwAAAAC1gaUgtWjRoktVBwAAAADUGhe12AQAAAAAXIkIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsMjd1QUAVsTFubqC/7dqlasrAAAAgKswIwUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLXBqkkpKS1KNHD/n7+ys4OFi33nqr9u3b59THMAwlJCQoLCxMPj4+6tu3r/bs2ePUp7CwUFOmTFFQUJD8/Pw0ZMgQHT9+/HKeCgAAAIAriEuD1KZNm3TPPfdo27ZtSk1N1dmzZzVgwACdOnXK7JOcnKzZs2dr/vz5SktLk8PhUGxsrPLy8sw+8fHxSklJ0bJly7R582bl5+dr8ODBKi4udsVpAQAAAKjjbIZhGK4uotT333+v4OBgbdq0Sddff70Mw1BYWJji4+M1c+ZMSb/OPoWEhGjWrFmaNGmScnJy1LhxY7355psaMWKEJOnkyZMKDw/Xxx9/rBtvvLHMcQoLC1VYWGg+z83NVXh4uHJychQQEHB5TvY84uJcXQEqY9UqV1cAAACA6pabmyu73X7BbFCj7pHKycmRJDVq1EiSdOjQIWVmZmrAgAFmHy8vL0VHR2vLli2SpB07dujMmTNOfcLCwhQVFWX2OVdSUpLsdrv5CA8Pv1SnBAAAAKAOqjFByjAMPfDAA+rTp4+ioqIkSZmZmZKkkJAQp74hISHmtszMTHl6eqphw4YV9jnXww8/rJycHPNx7Nix6j4dAAAAAHWYu6sLKHXvvffq66+/1ubNm8tss9lsTs8NwyjTdq7z9fHy8pKXl1fViwUAAABwRasRM1JTpkzRypUrtWHDBjVp0sRsdzgcklRmZikrK8ucpXI4HCoqKlJ2dnaFfQAAAACgOrk0SBmGoXvvvVfLly/X+vXrFRER4bQ9IiJCDodDqampZltRUZE2bdqkXr16SZK6desmDw8Ppz4ZGRnavXu32QcAAAAAqpNLL+2755579Pbbb+tf//qX/P39zZknu90uHx8f2Ww2xcfHKzExUZGRkYqMjFRiYqJ8fX01cuRIs++ECRM0bdo0BQYGqlGjRpo+fbo6dOig/v37u/L0AAAAANRRLg1SCxYskCT17dvXqX3RokUaN26cJGnGjBkqKCjQ5MmTlZ2drZ49e2rNmjXy9/c3+8+ZM0fu7u4aPny4CgoKFBMTo8WLF8vNze1ynQoAAACAK0iN+h4pV6nsWvGXC98jVTvwPVIAAAB1T638HikAAAAAqA0IUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYJFLg9R//vMfxcXFKSwsTDabTStWrHDabhiGEhISFBYWJh8fH/Xt21d79uxx6lNYWKgpU6YoKChIfn5+GjJkiI4fP34ZzwIAAADAlcalQerUqVPq1KmT5s+fX+725ORkzZ49W/Pnz1daWpocDodiY2OVl5dn9omPj1dKSoqWLVumzZs3Kz8/X4MHD1ZxcfHlOg0AAAAAVxibYRiGq4uQJJvNppSUFN16662Sfp2NCgsLU3x8vGbOnCnp19mnkJAQzZo1S5MmTVJOTo4aN26sN998UyNGjJAknTx5UuHh4fr444914403VurYubm5stvtysnJUUBAwCU5Pyvi4lxdASpj1SpXVwAAAIDqVtlsUGPvkTp06JAyMzM1YMAAs83Ly0vR0dHasmWLJGnHjh06c+aMU5+wsDBFRUWZfcpTWFio3NxcpwcAAAAAVFaNDVKZmZmSpJCQEKf2kJAQc1tmZqY8PT3VsGHDCvuUJykpSXa73XyEh4dXc/UAAAAA6rIaG6RK2Ww2p+eGYZRpO9eF+jz88MPKyckxH8eOHauWWgEAAABcGWpskHI4HJJUZmYpKyvLnKVyOBwqKipSdnZ2hX3K4+XlpYCAAKcHAAAAAFRWjQ1SERERcjgcSk1NNduKioq0adMm9erVS5LUrVs3eXh4OPXJyMjQ7t27zT4AAAAAUN3cXXnw/Px8/e9//zOfHzp0SOnp6WrUqJGaNm2q+Ph4JSYmKjIyUpGRkUpMTJSvr69GjhwpSbLb7ZowYYKmTZumwMBANWrUSNOnT1eHDh3Uv39/V50WAAAAgDrOpUFq+/bt6tevn/n8gQcekCSNHTtWixcv1owZM1RQUKDJkycrOztbPXv21Jo1a+Tv72++Zs6cOXJ3d9fw4cNVUFCgmJgYLV68WG5ubpf9fAAAAABcGWrM90i5Et8jharge6QAAADqnlr/PVIAAAAAUFMRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFrm7ugCgtoqLc3UF/2/VKldXAAAAcGVhRgoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggBQAAAAAWEaQAAAAAwCKCFAAAAABYRJACAAAAAIsIUgAAAABgkburCwBw8eLiXF3B/1u1ytUVAAAAXHrMSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBHfIwWgWvGdVgAA4EpQZ2akXn75ZUVERMjb21vdunXTp59+6uqSAAAAANRRdWJG6t1331V8fLxefvll9e7dW6+++qpuvvlmffPNN2ratKmrywMASczWAQBQl9gMwzBcXcTF6tmzp7p27aoFCxaYbe3atdOtt96qpKSkC74+NzdXdrtdOTk5CggIuJSlVkpN+mMLQN1EkCpfTfv9yzgBwOVX2WxQ62ekioqKtGPHDj300ENO7QMGDNCWLVvKfU1hYaEKCwvN5zk5OZJ+fdNqgjNnXF0BgLrupptcXcH/e+89V1fw/2ra798a8r8lAHXU8OGursBZTfn/QWkmuNB8U60PUj/88IOKi4sVEhLi1B4SEqLMzMxyX5OUlKQnn3yyTHt4ePglqREAUDG73dUV1Fy8NwCuJDXtd15eXp7s5ymq1gepUjabzem5YRhl2ko9/PDDeuCBB8znJSUl+umnnxQYGFjhay5Wbm6uwsPDdezYsRpx+SCqD2NbdzG2dRdjW3cxtnUb41t31aSxNQxDeXl5CgsLO2+/Wh+kgoKC5ObmVmb2KSsrq8wsVSkvLy95eXk5tTVo0OBSlegkICDA5R8OXBqMbd3F2NZdjG3dxdjWbYxv3VVTxvZ8M1Glav3y556enurWrZtSU1Od2lNTU9WrVy8XVQUAAACgLqv1M1KS9MADD2j06NHq3r27rrvuOi1cuFBHjx7V3Xff7erSAAAAANRBdSJIjRgxQj/++KOeeuopZWRkKCoqSh9//LGaNWvm6tJMXl5eeuKJJ8pcUojaj7GtuxjbuouxrbsY27qN8a27auPY1onvkQIAAACAy6nW3yMFAAAAAJcbQQoAAAAALCJIAQAAAIBFBCkAAAAAsIggVU2SkpJks9kUHx9vthmGoYSEBIWFhcnHx0d9+/bVnj17nF5XWFioKVOmKCgoSH5+fhoyZIiOHz9+matHeU6cOKFRo0YpMDBQvr6+6ty5s3bs2GFuZ3xrp7Nnz+rRRx9VRESEfHx81KJFCz311FMqKSkx+zC2tcN//vMfxcXFKSwsTDabTStWrHDaXl3jmJ2drdGjR8tut8tut2v06NH6+eefL/HZXdnON7ZnzpzRzJkz1aFDB/n5+SksLExjxozRyZMnnfbB2NZMF/q5/a1JkybJZrNp7ty5Tu2Mbc1UmbHdu3evhgwZIrvdLn9/f1177bU6evSoub22jS1BqhqkpaVp4cKF6tixo1N7cnKyZs+erfnz5ystLU0Oh0OxsbHKy8sz+8THxyslJUXLli3T5s2blZ+fr8GDB6u4uPhynwZ+Izs7W71795aHh4c++eQTffPNN3rhhRfUoEEDsw/jWzvNmjVLr7zyiubPn6+9e/cqOTlZzz33nF566SWzD2NbO5w6dUqdOnXS/Pnzy91eXeM4cuRIpaena/Xq1Vq9erXS09M1evToS35+V7Lzje3p06e1c+dOPfbYY9q5c6eWL1+u/fv3a8iQIU79GNua6UI/t6VWrFihzz//XGFhYWW2MbY104XG9uDBg+rTp4/atm2rjRs36quvvtJjjz0mb29vs0+tG1sDFyUvL8+IjIw0UlNTjejoaGPq1KmGYRhGSUmJ4XA4jGeffdbs+8svvxh2u9145ZVXDMMwjJ9//tnw8PAwli1bZvY5ceKEUa9ePWP16tWX9TzgbObMmUafPn0q3M741l6DBg0y7rrrLqe2YcOGGaNGjTIMg7GtrSQZKSkp5vPqGsdvvvnGkGRs27bN7LN161ZDkvHf//73Ep8VDKPs2Jbniy++MCQZR44cMQyDsa0tKhrb48ePG1dddZWxe/duo1mzZsacOXPMbYxt7VDe2I4YMcL8f215auPYMiN1ke655x4NGjRI/fv3d2o/dOiQMjMzNWDAALPNy8tL0dHR2rJliyRpx44dOnPmjFOfsLAwRUVFmX3gGitXrlT37t31hz/8QcHBwerSpYtee+01czvjW3v16dNH69at0/79+yVJX331lTZv3qyBAwdKYmzriuoax61bt8put6tnz55mn2uvvVZ2u52xrkFycnJks9nMqwYY29qrpKREo0eP1oMPPqj27duX2c7Y1k4lJSX66KOP1Lp1a914440KDg5Wz549nS7/q41jS5C6CMuWLdPOnTuVlJRUZltmZqYkKSQkxKk9JCTE3JaZmSlPT081bNiwwj5wjW+//VYLFixQZGSk/v3vf+vuu+/WfffdpzfeeEMS41ubzZw5U3fccYfatm0rDw8PdenSRfHx8brjjjskMbZ1RXWNY2ZmpoKDg8vsPzg4mLGuIX755Rc99NBDGjlypAICAiQxtrXZrFmz5O7urvvuu6/c7Yxt7ZSVlaX8/Hw9++yzuummm7RmzRoNHTpUw4YN06ZNmyTVzrF1v+xHrCOOHTumqVOnas2aNU7Xdp7LZrM5PTcMo0zbuSrTB5dWSUmJunfvrsTERElSly5dtGfPHi1YsEBjxowx+zG+tc+7776rpUuX6u2331b79u2Vnp6u+Ph4hYWFaezYsWY/xrZuqI5xLK8/Y10znDlzRrfffrtKSkr08ssvX7A/Y1uz7dixQ/PmzdPOnTstjwFjW7OVLuh0yy236P7775ckde7cWVu2bNErr7yi6OjoCl9bk8eWGakq2rFjh7KystStWze5u7vL3d1dmzZt0osvvih3d3fzX0HPTcdZWVnmNofDoaKiImVnZ1fYB64RGhqqq6++2qmtXbt25soyDodDEuNbGz344IN66KGHdPvtt6tDhw4aPXq07r//fnNmmbGtG6prHB0Oh7777rsy+//+++8Zaxc7c+aMhg8frkOHDik1NdWcjZIY29rq008/VVZWlpo2bWr+bXXkyBFNmzZNzZs3l8TY1lZBQUFyd3e/4N9WtW1sCVJVFBMTo127dik9Pd18dO/eXXfeeafS09PVokULORwOpaammq8pKirSpk2b1KtXL0lSt27d5OHh4dQnIyNDu3fvNvvANXr37q19+/Y5te3fv1/NmjWTJEVERDC+tdTp06dVr57zrz43NzfzX8sY27qhusbxuuuuU05Ojr744guzz+eff66cnBzG2oVKQ9SBAwe0du1aBQYGOm1nbGun0aNH6+uvv3b62yosLEwPPvig/v3vf0tibGsrT09P9ejR47x/W9XKsb3sy1vUYb9dtc8wDOPZZ5817Ha7sXz5cmPXrl3GHXfcYYSGhhq5ublmn7vvvtto0qSJsXbtWmPnzp3GDTfcYHTq1Mk4e/asC84Apb744gvD3d3deOaZZ4wDBw4Yb731luHr62ssXbrU7MP41k5jx441rrrqKuPDDz80Dh06ZCxfvtwICgoyZsyYYfZhbGuHvLw848svvzS+/PJLQ5Ixe/Zs48svvzRXbquucbzpppuMjh07Glu3bjW2bt1qdOjQwRg8ePBlP98ryfnG9syZM8aQIUOMJk2aGOnp6UZGRob5KCwsNPfB2NZMF/q5Pde5q/YZBmNbU11obJcvX254eHgYCxcuNA4cOGC89NJLhpubm/Hpp5+a+6htY0uQqkbnBqmSkhLjiSeeMBwOh+Hl5WVcf/31xq5du5xeU1BQYNx7771Go0aNDB8fH2Pw4MHG0aNHL3PlKM+qVauMqKgow8vLy2jbtq2xcOFCp+2Mb+2Um5trTJ061WjatKnh7e1ttGjRwnjkkUec/gBjbGuHDRs2GJLKPMaOHWsYRvWN448//mjceeedhr+/v+Hv72/ceeedRnZ29mU6yyvT+cb20KFD5W6TZGzYsMHcB2NbM13o5/Zc5QUpxrZmqszYvv7660arVq0Mb29vo1OnTsaKFSuc9lHbxtZmGIZxOWa+AAAAAKCu4B4pAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQBAjTdu3Djdeuut1b7fzMxMxcbGys/PTw0aNLisx74Umjdvrrlz5563j81m04oVKy5LPQBQlxGkAACSakZgOHz4sGw2m9LT0y/L8ebMmaOMjAylp6dr//795faZN2+eFi9efFnq+a3FixdXGO4qkpaWpj/96U+XpiAAgBN3VxcAAICrHDx4UN26dVNkZGSFfex2+2Ws6OI0btzY1SUAwBWDGSkAQKV88803GjhwoOrXr6+QkBCNHj1aP/zwg7m9b9++uu+++zRjxgw1atRIDodDCQkJTvv473//qz59+sjb21tXX3211q5d63SpWUREhCSpS5custls6tu3r9Prn3/+eYWGhiowMFD33HOPzpw5c96aFyxYoJYtW8rT01Nt2rTRm2++aW5r3ry5/vnPf+qNN96QzWbTuHHjyt3HuTN1lTlPm82mBQsW6Oabb5aPj48iIiL0/vvvm9s3btwom82mn3/+2WxLT0+XzWbT4cOHtXHjRo0fP145OTmy2Wyy2WxljlGecy/tO3DggK6//nrz/U5NTXXqX1RUpHvvvVehoaHy9vZW8+bNlZSUdMHjAAAIUgCASsjIyFB0dLQ6d+6s7du3a/Xq1fruu+80fPhwp35LliyRn5+fPv/8cyUnJ+upp54y/3gvKSnRrbfeKl9fX33++edauHChHnnkEafXf/HFF5KktWvXKiMjQ8uXLze3bdiwQQcPHtSGDRu0ZMkSLV68+LyX3KWkpGjq1KmaNm2adu/erUmTJmn8+PHasGGDpF8vg7vppps0fPhwZWRkaN68eZV+P853nqUee+wx3Xbbbfrqq680atQo3XHHHdq7d2+l9t+rVy/NnTtXAQEBysjIUEZGhqZPn17p+qRf3+9hw4bJzc1N27Zt0yuvvKKZM2c69XnxxRe1cuVKvffee9q3b5+WLl2q5s2bWzoOAFypuLQPAHBBCxYsUNeuXZWYmGi2/eMf/1B4eLj279+v1q1bS5I6duyoJ554QpIUGRmp+fPna926dYqNjdWaNWt08OBBbdy4UQ6HQ5L0zDPPKDY21txn6aVpgYGBZp9SDRs21Pz58+Xm5qa2bdtq0KBBWrdunf74xz+WW/Pzzz+vcePGafLkyZKkBx54QNu2bdPzzz+vfv36qXHjxvLy8pKPj0+ZY13I+c6z1B/+8AdNnDhRkvTXv/5Vqampeumll/Tyyy9fcP+enp6y2+2y2WyWayu1du1a7d27V4cPH1aTJk0kSYmJibr55pvNPkePHlVkZKT69Okjm82mZs2aVelYAHAlYkYKAHBBO3bs0IYNG1S/fn3z0bZtW0m/3mdUqmPHjk6vCw0NVVZWliRp3759Cg8PdwoG11xzTaVraN++vdzc3Mrdd3n27t2r3r17O7X17t270rNC53O+8yx13XXXlXleHceurL1796pp06ZmiCqvpnHjxik9PV1t2rTRfffdpzVr1ly2+gCgtmNGCgBwQSUlJYqLi9OsWbPKbAsNDTX/28PDw2mbzWZTSUmJJMkwDNlstirXcL59V+Tc411sDRdTy2/rqVevnllPqQvd72XVb/d97vFLde3aVYcOHdInn3yitWvXavjw4erfv78++OCDaq0FAOoiZqQAABfUtWtX7dmzR82bN1erVq2cHn5+fpXaR9u2bXX06FF99913ZltaWppTH09PT0lScXHxRdfcrl07bd682alty5Ytateu3UXvuzK2bdtW5nnpLF7pJYwZGRnm9nOXfPf09Lyo9+Hqq6/W0aNHdfLkSbNt69atZfoFBARoxIgReu211/Tuu+/qn//8p3766acqHxcArhTMSAEATDk5OWX+oG/UqJHuuecevfbaa7rjjjv04IMPKigoSP/73/+0bNkyvfbaa06X3FUkNjZWLVu21NixY5WcnKy8vDxzsYnSmZLg4GD5+Pho9erVatKkiby9vau8/PiDDz6o4cOHq2vXroqJidGqVau0fPlyrV27tkr7s+r9999X9+7d1adPH7311lv64osv9Prrr0uSWrVqpfDwcCUkJOjpp5/WgQMH9MILLzi9vnnz5srPz9e6devUqVMn+fr6ytfXt9LH79+/v9q0aaMxY8bohRdeUG5ubpnFPebMmaPQ0FB17txZ9erV0/vvvy+Hw2H5+6sA4ErEjBQAwLRx40Z16dLF6fH4448rLCxMn332mYqLi3XjjTcqKipKU6dOld1uNy9TuxA3NzetWLFC+fn56tGjhyZOnKhHH31UkuTt7S1Jcnd314svvqhXX31VYWFhuuWWW6p8LrfeeqvmzZun5557Tu3bt9err76qRYsWlVlS/VJ58skntWzZMnXs2FFLlizRW2+9pauvvlrSr5cGvvPOO/rvf/+rTp06adasWXr66aedXt+rVy/dfffdGjFihBo3bqzk5GRLx69Xr55SUlJUWFioa665RhMnTtQzzzzj1Kd+/fqaNWuWunfvrh49eujw4cP6+OOPKz2mAHAlsxnlXUQNAMBl8Nlnn6lPnz763//+p5YtW7q6nGpjs9mUkpLi9P1TAIC6hUv7AACXTUpKiurXr6/IyEj973//09SpU9W7d+86FaIAAFcGghQA4LLJy8vTjBkzdOzYMQUFBal///5l7g1C+T799FOn74A6V35+/mWsBgDApX0AANQCBQUFOnHiRIXbW7VqdRmrAQAQpAAAAADAIpblAQAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAov8DHDpiRtvlhC4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 700 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c308af3ab24fb38dcafafbf7723c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 16063, 261, 7458, 4817, 28706, 481, 2880, 12874, 639, 2245, 28709, 430, 2006, 28717, 296, 1452, 337, 4210, 263, 13693, 385, 3753, 9290, 385, 28725, 24330, 758, 26420, 292, 328, 385, 3459, 3958, 3118, 19931, 15698, 28723, 1136, 603, 28708, 264, 14150, 13693, 28709, 543, 911, 2741, 1632, 10384, 8783, 2388, 1540, 481, 543, 5227, 2895, 955, 2904, 28708, 28723, 21073, 28708, 1515, 1204, 3482, 481, 1221, 1827, 9292, 28723, 9134, 20577, 12697, 340, 911, 2741, 299, 1452, 1966, 28747, 13, 13, 28780, 4104, 28747, 2674, 8358, 385, 289, 936, 11520, 3303, 1485, 4306, 293, 28723, 13, 20536, 962, 28747, 650, 25786, 332, 24339, 385, 290, 831, 296, 3482, 28723, 13, 20536, 1020, 28747, 5158, 455, 274, 1016, 13995, 3482, 379, 639, 17925, 28723, 13, 28780, 11724, 28747, 393, 786, 4585, 290, 831, 296, 3482, 28723, 13, 20536, 28802, 28747, 334, 1899, 293, 289, 10717, 2402, 28723, 13, 24001, 28747, 2213, 11234, 289, 21065, 350, 385, 2283, 872, 385, 28723, 13, 13, 6570, 10125, 1037, 521, 28324, 28709, 28747, 13, 13, 3104, 28747, 384, 385, 281, 12697, 28725, 3459, 3958, 6700, 3557, 1452, 4882, 281, 12697, 11545, 955, 318, 2892, 25062, 9584, 28717, 10608, 481, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 3833, 2780, 2567, 955, 264, 20482, 15546, 283, 9927, 28725, 3913, 20594, 337, 25403, 28725, 521, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 2646, 281, 1331, 385, 27947, 340, 955, 2635, 4498, 361, 293, 340, 318, 2892, 25062, 295, 323, 269, 957, 11174, 20423, 17154, 2892, 955, 22544, 955, 981, 2220, 4498, 4807, 16262, 2635, 1908, 293, 15807, 16779, 293, 7445, 15985, 28747, 12012, 28780, 4104, 1869, 5936, 28735, 2892, 25062, 647, 464, 28708, 20482, 647, 464, 450, 318, 2892, 25062, 5807, 464, 28780, 11724, 1869, 5936, 269, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 5807, 464, 20536, 1020, 1869, 5936, 23718, 20594, 337, 25403, 5807, 464, 20536, 962, 1869, 5936, 370, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 647, 464, 11215, 4498, 361, 293, 647, 464, 3368, 17154, 2892, 955, 22544, 1421, 28752, 13, 13, 28741, 18701, 28725, 2691, 28708, 543, 19846, 8783, 261, 7458, 28747, 13, 13, 3104, 28747, 981, 19281, 15807, 19538, 9879, 361, 934, 293, 6552, 293, 28725, 281, 1452, 955, 10404, 322, 2737, 934, 14481, 481, 521, 19131, 15807, 17686, 28725, 679, 17914, 4722, 337, 23017, 2567, 6649, 930, 16475, 2646, 543, 679, 831, 1762, 340, 1515, 14986, 11829, 12893, 1150, 274, 289, 277, 615, 293, 955, 11176, 2169, 2567, 639, 18654, 9350, 5233, 4533, 28825, 639, 3197, 24307, 340, 4902, 554, 28723, 15985, 28747, 12012, 24001, 1869, 3635, 28725, 464, 20536, 962, 1869, 3635, 28725, 464, 20536, 1020, 1869, 3635, 28725, 464, 28780, 11724, 1869, 3635, 28725, 464, 28780, 4104, 1869, 5936, 301, 3197, 24307, 340, 4902, 554, 5807, 464, 20536, 28802, 1869, 3635, 28752, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 10:50:08.836206: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: “Es muy importante seguir estas normas, dado que nosotros estamos en un momento muy especial, conteniendo y trabajando fuertemente para la contención de los diferentes linajes o cepas que está presentando el mundo”, concluyó el Ministro de Salud. Output: {'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['el Ministro de Salud'], 'WHY': None}</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[2]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset['input'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
    "\n",
    "WHO: Sujetos o entidades involucradas.\n",
    "WHAT: Hechos u objetos mencionados.\n",
    "WHEN: Detalles relacionados con el tiempo.\n",
    "WHERE: Lugares mencionados.\n",
    "WHY: Causas o razones.\n",
    "HOW: Maneras o métodos descritos.\n",
    "\n",
    "Abajo es un ejemplo:\n",
    "\n",
    "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
    "\n",
    "Ahora, completa la siguiente tarea:\n",
    "\n",
    "Input: Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos. Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
      "\n",
      "### Output:\n",
      "[\n",
      "    {\n",
      "        \"text\": \"Gobierno\",\n",
      "        \"label\": \"WHAT\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"subvenciona\",\n",
      "        \"label\": \"WHAT\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"estudio\",\n",
      "        \"label\": \"WHAT\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"impacto\",\n",
      "        \"label\": \"WHAT\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"sexista\",\n",
      "        \"label\": \"WHAT\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"pirrope\",\n",
      "        \"label\": \"WHAT\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOW': ['con 45.980 euros'],\n",
       " 'WHAT': ['un estudio sobre el impacto sexista de los piropos'],\n",
       " 'WHEN': None,\n",
       " 'WHERE': ['en un artículo que puedes leer en este enlace'],\n",
       " 'WHO': ['el digital OK Diario', 'el Gobierno de España'],\n",
       " 'WHY': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/FLARE-Challenge/Mistral-models/wandb/run-20240509_203458-j1no7tuc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiayunliu2000/Flares-finetune/runs/j1no7tuc' target=\"_blank\">mistral-7b-flare-finetune-train-split-JSON-Template2-2024-05-09-20-34</a></strong> to <a href='https://wandb.ai/jiayunliu2000/Flares-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiayunliu2000/Flares-finetune' target=\"_blank\">https://wandb.ai/jiayunliu2000/Flares-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiayunliu2000/Flares-finetune/runs/j1no7tuc' target=\"_blank\">https://wandb.ai/jiayunliu2000/Flares-finetune/runs/j1no7tuc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='1902' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 140/1902 03:17 < 41:58, 0.70 it/s, Epoch 0.22/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.285435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 37\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3147\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2124\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2124\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"flare-finetune-train-split-JSON-Template1\"\n",
    "base_model_name = \"mistral-7b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=100,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model_path = \"mistral7b-flare-finetune-train-split-JSON-Template1\"\n",
    "ft_model = PeftModel.from_pretrained(base_model, f\"{model_path}/checkpoint-1900\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "Set stopping criteria to stop the model generating garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"}\", \"'}}\", \"'}\\n\", \"}}\\n\", \"'}\\n\\n\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
    "\n",
    "WHO: Sujetos o entidades involucradas.\n",
    "WHAT: Hechos u objetos mencionados.\n",
    "WHEN: Detalles relacionados con el tiempo.\n",
    "WHERE: Lugares mencionados.\n",
    "WHY: Causas o razones.\n",
    "HOW: Maneras o métodos descritos.\n",
    "\n",
    "Abajo es un ejemplo:\n",
    "\n",
    "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
    "\n",
    "Ahora, completa la siguiente tarea:\n",
    "\n",
    "Input: Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos. Output:\n",
    "\"\"\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### 7. FLARE Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 TASK\n",
    "print(tokenized_train_dataset.shape)\n",
    "print(tokenized_val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = f\"{tokenized_val_dataset[0]['input']} Output:\"\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    model_input = eval_tokenizer(model_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val_dataset[0]['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the generated text transform to FLARE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "ft_model.eval()\n",
    "res = []\n",
    "\n",
    "start_time = time.time()  # Start timing before the loop\n",
    "\n",
    "for x in tokenized_val_dataset:\n",
    "    prompt = f\"{x['input']} Output: \"\n",
    "    model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]\n",
    "        input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "        generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "        res.append(generated_text)\n",
    "        \n",
    "end_time = time.time()  # End timing after the loop\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time spent generating text: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(f\"{model_path}/results_1900iter.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(res, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistral7b-flare-finetune-train-split-JSON-Template1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load results\n",
    "with open(f\"{model_path}/results_1900iter.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    res = json.load(file)\n",
    "\n",
    "# Load the data\n",
    "task1_train = pd.read_json('../Flares-dataset/5w1h_subtarea_1_train_train.json', lines=True)\n",
    "task1_test = pd.read_json('../Flares-dataset/5w1h_subtarea_1_train_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index_format(data_dict, original_text):\n",
    "    tags_list = []\n",
    "\n",
    "    for label, fragments in data_dict.items():\n",
    "        if fragments is not None:\n",
    "            for fragment in fragments:\n",
    "                start_index = original_text.find(fragment)\n",
    "                if start_index != -1:\n",
    "                    end_index = start_index + len(fragment)\n",
    "                    tags_list.append({\n",
    "                        'Tag_Start': start_index,\n",
    "                        'Tag_End': end_index,\n",
    "                        '5W1H_Label': label,\n",
    "                        'Tag_Text': fragment\n",
    "                    })\n",
    "    \n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El experto ha acompañado el mensaje con una fotografía de uno de los casos observados, en el que se ve una lengua manchada o descolorida.\\xa0'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_test['Text'][250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros\\xa0un estudio sobre el impacto sexista de los piropos.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_train['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['la subdirectora general de Promoción de la Salud', 'desde el departamento catalán'], 'WHY': None}\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Initialize an empty list to store the formatted lists\n",
    "all_formatted_lists = []\n",
    "\n",
    "# Assuming 'convert_to_index_format' is a function and 'task1_test' is a dataset with a column 'Text'\n",
    "for i in range(len(task1_test['Text'])):\n",
    "    try:\n",
    "        # Convert the current item and get the formatted list\n",
    "        formatted_list = convert_to_index_format(ast.literal_eval(res[i]), task1_test['Text'][i])\n",
    "\n",
    "        # Append the formatted list to the collection of all formatted lists\n",
    "        all_formatted_lists.append(formatted_list)\n",
    "        \n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        # If an error occurs, simply skip this item and continue with the next\n",
    "        all_formatted_lists.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting each sublist by 'Tag_Start' key\n",
    "sorted_all_formatted_lists = [sorted(sub_list, key=lambda x: x['Tag_Start']) for sub_list in all_formatted_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Generated tags\n",
      "[{'5W1H_Label': 'WHO',\n",
      "  'Tag_End': 88,\n",
      "  'Tag_Start': 40,\n",
      "  'Tag_Text': 'la subdirectora general de Promoción de la Salud'},\n",
      " {'5W1H_Label': 'WHO',\n",
      "  'Tag_End': 140,\n",
      "  'Tag_Start': 111,\n",
      "  'Tag_Text': 'desde el departamento catalán'}]\n",
      "####Ground Truh tags\n",
      "[{'5W1H_Label': 'WHO',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 88,\n",
      "  'Tag_Start': 40,\n",
      "  'Tag_Text': 'la subdirectora general de Promoción de la Salud'},\n",
      " {'5W1H_Label': 'WHERE',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 140,\n",
      "  'Tag_Start': 111,\n",
      "  'Tag_Text': 'desde el departamento catalán'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 218,\n",
      "  'Tag_Start': 152,\n",
      "  'Tag_Text': 'que no tendría que haber un límite de edad vinculado a esta '\n",
      "              'vacuna'}]\n",
      "####Generated tags\n",
      "[{'5W1H_Label': 'WHAT', 'Tag_End': 20, 'Tag_Start': 14, 'Tag_Text': 'dormir'},\n",
      " {'5W1H_Label': 'WHEN',\n",
      "  'Tag_End': 58,\n",
      "  'Tag_Start': 34,\n",
      "  'Tag_Text': 'entre 6 y 8 horas al día'},\n",
      " {'5W1H_Label': 'WHEN',\n",
      "  'Tag_End': 106,\n",
      "  'Tag_Start': 77,\n",
      "  'Tag_Text': 'en las actividades cotidianas'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 148,\n",
      "  'Tag_Start': 134,\n",
      "  'Tag_Text': 'el metabolismo'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 189,\n",
      "  'Tag_Start': 164,\n",
      "  'Tag_Text': 'los alimentos se digieren'},\n",
      " {'5W1H_Label': 'HOW',\n",
      "  'Tag_End': 201,\n",
      "  'Tag_Start': 190,\n",
      "  'Tag_Text': 'mucho mejor'}]\n",
      "####Ground Truh tags\n",
      "[{'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 20,\n",
      "  'Tag_Start': 14,\n",
      "  'Tag_Text': 'dormir'},\n",
      " {'5W1H_Label': 'HOW',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 51,\n",
      "  'Tag_Start': 21,\n",
      "  'Tag_Text': 'por lo menos entre 6 y 8 horas'},\n",
      " {'5W1H_Label': 'WHEN',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 58,\n",
      "  'Tag_Start': 52,\n",
      "  'Tag_Text': 'al día'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 106,\n",
      "  'Tag_Start': 77,\n",
      "  'Tag_Text': 'en las actividades cotidianas'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 123,\n",
      "  'Tag_Start': 115,\n",
      "  'Tag_Text': 'el humor'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 148,\n",
      "  'Tag_Start': 134,\n",
      "  'Tag_Text': 'el metabolismo'},\n",
      " {'5W1H_Label': 'WHY',\n",
      "  'Reliability_Label': 'semiconfiable',\n",
      "  'Tag_End': 201,\n",
      "  'Tag_Start': 150,\n",
      "  'Tag_Text': 'de manera que los alimentos se digieren mucho mejor'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1, width=80, depth=None, compact=False)\n",
    "for i in range(0,2):\n",
    "    print(f\"####Generated tags\")\n",
    "    pp.pprint(sorted_all_formatted_lists[i])\n",
    "    print(f\"####Ground Truh tags\")\n",
    "    pp.pprint(tokenized_val_dataset[i]['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract both 'tags' and 'Id' from each entry and store them in a list of dictionaries\n",
    "ground_truth = [{'Id': entry['Id'], 'Tags': f\"{entry['tags']}\"} for entry in tokenized_val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': 1476,\n",
       " 'Tags': \"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\"}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVirginia Evaluation code\\n# Generated using ChatGPT\\n\\ndef evaluate_spans(predicted_spans, gold_spans):\\n    # Initialize counters for different types of matches\\n    total_correct_matches = 0\\n    total_incorrect_matches = 0\\n    total_partial_matches = 0\\n    total_missing_matches = 0\\n    total_spurious_matches = 0\\n\\n    # Iterate over each pair of predicted and gold documents\\n    for predicted_spans, gold_spans in zip(predicted_spans, gold_spans):\\n        correct_matches = 0\\n        incorrect_matches = 0\\n        partial_matches = 0\\n        missing_matches = len(gold_spans)  # Initialize with total gold spans for this document\\n        spurious_matches = 0\\n\\n        # Iterate through each gold span in the current document\\n        for gold_span in gold_spans:\\n            found_match = False  # Flag to track if a match is found for the current gold span\\n\\n            # Iterate through each predicted span in the current document\\n            for predicted_span in predicted_spans:\\n                # Check if start and end indices and labels match exactly\\n                if (gold_span[\\'Tag_Start\\'] == predicted_span[\\'Tag_Start\\'] and\\n                    gold_span[\\'Tag_End\\'] == predicted_span[\\'Tag_End\\'] and\\n                    gold_span[\\'5W1H_Label\\'] == predicted_span[\\'5W1H_Label\\']):\\n                    correct_matches += 1\\n                    found_match = True\\n                    break\\n\\n                # Check if start and end indices match but labels don\\'t\\n                elif (gold_span[\\'Tag_Start\\'] == predicted_span[\\'Tag_Start\\'] and\\n                      gold_span[\\'Tag_End\\'] == predicted_span[\\'Tag_End\\']):\\n                    incorrect_matches += 1\\n                    found_match = True\\n                    break\\n\\n                # Check for partial matches\\n                elif (gold_span[\\'Tag_Start\\'] <= predicted_span[\\'Tag_End\\'] and\\n                      gold_span[\\'Tag_End\\'] >= predicted_span[\\'Tag_Start\\']):\\n                    partial_matches += 1\\n                    found_match = True\\n                    break\\n\\n            # If no match is found, decrement the missing match counter\\n            if found_match:\\n                missing_matches -= 1\\n\\n        # Calculate spurious matches for the current document\\n        spurious_matches = len(predicted_spans) - (correct_matches + incorrect_matches + partial_matches)\\n\\n        # Accumulate document-level results into total results\\n        total_correct_matches += correct_matches\\n        total_incorrect_matches += incorrect_matches\\n        total_partial_matches += partial_matches\\n        total_missing_matches += missing_matches\\n        total_spurious_matches += spurious_matches\\n\\n    return total_correct_matches, total_incorrect_matches, total_partial_matches, total_missing_matches, total_spurious_matches\\n\\n\\n# Example usage:\\ngold_spans = [{\"Tag_Start\":5,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"los investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHAT\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":102,\"Tag_End\":146,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\\npredicted_spans = [{\"Tag_Start\":9,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHERE\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":24,\"Tag_End\":38,\"5W1H_Label\":\"HOW\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\\n\\n# Call the function with gold and predicted spans\\ncorrect, incorrect, partial, missing, spurious = evaluate_spans(sorted_all_formatted_lists, ground_truth)\\n\\n# Print the evaluation metrics\\nprint(\"Correct Matches:\", correct)\\nprint(\"Incorrect Matches:\", incorrect)\\nprint(\"Partial Matches:\", partial)\\nprint(\"Missing Matches:\", missing)\\nprint(\"Spurious Matches:\", spurious)\\n\\nrecall = (correct + (0.5 * partial)) / (correct + incorrect + partial + missing)\\n\\nprecision = (correct + (0.5 * partial)) / (correct + incorrect + partial + spurious)\\n\\nf1 = 2 * (precision * recall) / (precision + recall)\\n\\nprint(f\"Recall: {recall}, Precision: {precision}, F1: {f1}\")\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Virginia Evaluation code\n",
    "# Generated using ChatGPT\n",
    "\n",
    "def evaluate_spans(predicted_spans, gold_spans):\n",
    "    # Initialize counters for different types of matches\n",
    "    total_correct_matches = 0\n",
    "    total_incorrect_matches = 0\n",
    "    total_partial_matches = 0\n",
    "    total_missing_matches = 0\n",
    "    total_spurious_matches = 0\n",
    "\n",
    "    # Iterate over each pair of predicted and gold documents\n",
    "    for predicted_spans, gold_spans in zip(predicted_spans, gold_spans):\n",
    "        correct_matches = 0\n",
    "        incorrect_matches = 0\n",
    "        partial_matches = 0\n",
    "        missing_matches = len(gold_spans)  # Initialize with total gold spans for this document\n",
    "        spurious_matches = 0\n",
    "\n",
    "        # Iterate through each gold span in the current document\n",
    "        for gold_span in gold_spans:\n",
    "            found_match = False  # Flag to track if a match is found for the current gold span\n",
    "\n",
    "            # Iterate through each predicted span in the current document\n",
    "            for predicted_span in predicted_spans:\n",
    "                # Check if start and end indices and labels match exactly\n",
    "                if (gold_span['Tag_Start'] == predicted_span['Tag_Start'] and\n",
    "                    gold_span['Tag_End'] == predicted_span['Tag_End'] and\n",
    "                    gold_span['5W1H_Label'] == predicted_span['5W1H_Label']):\n",
    "                    correct_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "                # Check if start and end indices match but labels don't\n",
    "                elif (gold_span['Tag_Start'] == predicted_span['Tag_Start'] and\n",
    "                      gold_span['Tag_End'] == predicted_span['Tag_End']):\n",
    "                    incorrect_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "                # Check for partial matches\n",
    "                elif (gold_span['Tag_Start'] <= predicted_span['Tag_End'] and\n",
    "                      gold_span['Tag_End'] >= predicted_span['Tag_Start']):\n",
    "                    partial_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            # If no match is found, decrement the missing match counter\n",
    "            if found_match:\n",
    "                missing_matches -= 1\n",
    "\n",
    "        # Calculate spurious matches for the current document\n",
    "        spurious_matches = len(predicted_spans) - (correct_matches + incorrect_matches + partial_matches)\n",
    "\n",
    "        # Accumulate document-level results into total results\n",
    "        total_correct_matches += correct_matches\n",
    "        total_incorrect_matches += incorrect_matches\n",
    "        total_partial_matches += partial_matches\n",
    "        total_missing_matches += missing_matches\n",
    "        total_spurious_matches += spurious_matches\n",
    "\n",
    "    return total_correct_matches, total_incorrect_matches, total_partial_matches, total_missing_matches, total_spurious_matches\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "gold_spans = [{\"Tag_Start\":5,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"los investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHAT\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":102,\"Tag_End\":146,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\n",
    "predicted_spans = [{\"Tag_Start\":9,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHERE\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":24,\"Tag_End\":38,\"5W1H_Label\":\"HOW\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\n",
    "\n",
    "# Call the function with gold and predicted spans\n",
    "correct, incorrect, partial, missing, spurious = evaluate_spans(sorted_all_formatted_lists, ground_truth)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Correct Matches:\", correct)\n",
    "print(\"Incorrect Matches:\", incorrect)\n",
    "print(\"Partial Matches:\", partial)\n",
    "print(\"Missing Matches:\", missing)\n",
    "print(\"Spurious Matches:\", spurious)\n",
    "\n",
    "recall = (correct + (0.5 * partial)) / (correct + incorrect + partial + missing)\n",
    "\n",
    "precision = (correct + (0.5 * partial)) / (correct + incorrect + partial + spurious)\n",
    "\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Recall: {recall}, Precision: {precision}, F1: {f1}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3873642842.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [137], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    Recall: {recall:.4f}\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the text content\n",
    "metrics_content = f\"\"\"\n",
    "Correct Matches: {correct}\n",
    "Incorrect Matches: {incorrect}\n",
    "Partial Matches: {partial}\n",
    "Missing Matches: {missing}\n",
    "Spurious Matches: {spurious}\n",
    "Recall: {recall:.4f}\n",
    "Precision: {precision:.4f}\n",
    "F1 Score: {f1:.4f}\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "with open(f\"{model_path}/evaluation_metrics.txt\", \"w\") as file:\n",
    "    file.write(metrics_content)\n",
    "\n",
    "print(\"Metrics have been saved to 'evaluation_metrics.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Tags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using the IDs and the corresponding formatted tag lists\n",
    "tags_df = pd.DataFrame({\n",
    "    'Id': task1_test['Id'],\n",
    "    'Tags': [str(tags) for tags in sorted_all_formatted_lists]  # Convert each list to a string and enclose it in single quotes\n",
    "})\n",
    "\n",
    "print(tags_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "tags_df.to_csv(f\"{model_path}/tags_data_1900iter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': 1476,\n",
       " 'Tags': \"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\"}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.DataFrame(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "ground_truth.to_csv(f\"{model_path}/ground_truth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth['Tags'][135]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST FLARE CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                                               Tags\n",
      "0    1382  [{'Tag_Start': 40, 'Tag_End': 88, '5W1H_Label'...\n",
      "1     553  [{'Tag_Start': 14, 'Tag_End': 20, '5W1H_Label'...\n",
      "2     567  [{'Tag_Start': 0, 'Tag_End': 52, '5W1H_Label':...\n",
      "3     483  [{'Tag_Start': 15, 'Tag_End': 23, '5W1H_Label'...\n",
      "4     938  [{'Tag_Start': 6, 'Tag_End': 33, '5W1H_Label':...\n",
      "..    ...                                                ...\n",
      "312  1447  [{'Tag_Start': 5, 'Tag_End': 18, '5W1H_Label':...\n",
      "313   583  [{'Tag_Start': 197, 'Tag_End': 221, '5W1H_Labe...\n",
      "314   766                                                 []\n",
      "315  1097  [{'Tag_Start': 38, 'Tag_End': 62, '5W1H_Label'...\n",
      "316   789  [{'Tag_Start': 20, 'Tag_End': 27, '5W1H_Label'...\n",
      "\n",
      "[317 rows x 2 columns]\n",
      "[{'Precision': 0.638142747505756, 'Recall': 0.5859760394644116, 'F1': 0.6109478324761205, 'Accuracy': 0.4917208752217623}]\n"
     ]
    }
   ],
   "source": [
    "!python \"../evaluate_subtask_1.py\" --pathDataGold /mistral7b-flare-finetune-train-split-JSON-Template1/ground_truth.csv --pathDataInfered /mistral7b-flare-finetune-train-split-JSON-Template1/tags_data_1900iter.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
