{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```\n",
    "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='../data_files/json_format1/5w1h_subtask_1_zero_train_train_json_format1.json', split='train')\n",
    "#eval_dataset = load_dataset('json', data_files='../data_files/5w1h_subtask_1_zero_test_json_format1.json', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='../data_files/json_format1/5w1h_subtask_1_test_json_format1.json', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator ([description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f-fJR0MlQiTD"
   },
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"{example['input']} Output:\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"{example['input']} Output: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Mistral - mistralai/Mistral-7B-v0.1 - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JupyterLab v3.4.6\n",
      "/opt/conda/share/jupyter/labextensions\n",
      "        jupyterlab_pygments v0.2.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_pygments)\n",
      "        jupyter-matplotlib v0.11.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m\n",
      "        @jupyter-widgets/jupyterlab-manager v5.0.3 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_widgets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411006f8b82842759f20f34909bdb1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 700 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b791b0fbbe544ef7965d434b6c3e2e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/402 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "OKHhvxK83m19",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 16063, 261, 7458, 4817, 28706, 481, 2880, 12874, 639, 2245, 28709, 430, 2006, 28717, 296, 1452, 337, 4210, 263, 13693, 385, 3753, 9290, 385, 28725, 24330, 758, 26420, 292, 328, 385, 3459, 3958, 3118, 19931, 15698, 28723, 1136, 603, 28708, 264, 14150, 13693, 28709, 543, 911, 2741, 1632, 10384, 8783, 2388, 1540, 481, 543, 5227, 2895, 955, 2904, 28708, 28723, 21073, 28708, 1515, 1204, 3482, 481, 1221, 1827, 9292, 28723, 9134, 20577, 12697, 340, 911, 2741, 299, 1452, 1966, 28747, 13, 13, 28780, 4104, 28747, 2674, 8358, 385, 289, 936, 11520, 3303, 1485, 4306, 293, 28723, 13, 20536, 962, 28747, 650, 25786, 332, 24339, 385, 290, 831, 296, 3482, 28723, 13, 20536, 1020, 28747, 5158, 455, 274, 1016, 13995, 3482, 379, 639, 17925, 28723, 13, 28780, 11724, 28747, 393, 786, 4585, 290, 831, 296, 3482, 28723, 13, 20536, 28802, 28747, 334, 1899, 293, 289, 10717, 2402, 28723, 13, 24001, 28747, 2213, 11234, 289, 21065, 350, 385, 2283, 872, 385, 28723, 13, 13, 6570, 10125, 1037, 521, 28324, 28709, 28747, 13, 13, 3104, 28747, 384, 385, 281, 12697, 28725, 3459, 3958, 6700, 3557, 1452, 4882, 281, 12697, 11545, 955, 318, 2892, 25062, 9584, 28717, 10608, 481, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 3833, 2780, 2567, 955, 264, 20482, 15546, 283, 9927, 28725, 3913, 20594, 337, 25403, 28725, 521, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 2646, 281, 1331, 385, 27947, 340, 955, 2635, 4498, 361, 293, 340, 318, 2892, 25062, 295, 323, 269, 957, 11174, 20423, 17154, 2892, 955, 22544, 955, 981, 2220, 4498, 4807, 16262, 2635, 1908, 293, 15807, 16779, 293, 7445, 15985, 28747, 12012, 28780, 4104, 1869, 5936, 28735, 2892, 25062, 647, 464, 28708, 20482, 647, 464, 450, 318, 2892, 25062, 5807, 464, 28780, 11724, 1869, 5936, 269, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 5807, 464, 20536, 1020, 1869, 5936, 23718, 20594, 337, 25403, 5807, 464, 20536, 962, 1869, 5936, 370, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 647, 464, 11215, 4498, 361, 293, 647, 464, 3368, 17154, 2892, 955, 22544, 1421, 28752, 13, 13, 28741, 18701, 28725, 2691, 28708, 543, 19846, 8783, 261, 7458, 28747, 13, 13, 3104, 28747, 981, 19281, 15807, 19538, 9879, 361, 934, 293, 6552, 293, 28725, 281, 1452, 955, 10404, 322, 2737, 934, 14481, 481, 521, 19131, 15807, 17686, 28725, 679, 17914, 4722, 337, 23017, 2567, 6649, 930, 16475, 2646, 543, 679, 831, 1762, 340, 1515, 14986, 11829, 12893, 1150, 274, 289, 277, 615, 293, 955, 11176, 2169, 2567, 639, 18654, 9350, 5233, 4533, 28825, 639, 3197, 24307, 340, 4902, 554, 28723, 15985, 28747, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:18:48.987111: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-16 14:18:49.993909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 14:18:51.559831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: “Es muy importante seguir estas normas, dado que nosotros estamos en un momento muy especial, conteniendo y trabajando fuertemente para la contención de los diferentes linajes o cepas que está presentando el mundo”, concluyó el Ministro de Salud. Output:</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[2]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1670\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Id', 'output', 'input', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1268\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['input'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 16063, 261, 7458, 4817, 28706, 481, 2880, 12874, 639, 2245, 28709, 430, 2006, 28717, 296, 1452, 337, 4210, 263, 13693, 385, 3753, 9290, 385, 28725, 24330, 758, 26420, 292, 328, 385, 3459, 3958, 3118, 19931, 15698, 28723, 1136, 603, 28708, 264, 14150, 13693, 28709, 543, 911, 2741, 1632, 10384, 8783, 2388, 1540, 481, 543, 5227, 2895, 955, 2904, 28708, 28723, 21073, 28708, 1515, 1204, 3482, 481, 1221, 1827, 9292, 28723, 9134, 20577, 12697, 340, 911, 2741, 299, 1452, 1966, 28747, 13, 13, 28780, 4104, 28747, 2674, 8358, 385, 289, 936, 11520, 3303, 1485, 4306, 293, 28723, 13, 20536, 962, 28747, 650, 25786, 332, 24339, 385, 290, 831, 296, 3482, 28723, 13, 20536, 1020, 28747, 5158, 455, 274, 1016, 13995, 3482, 379, 639, 17925, 28723, 13, 28780, 11724, 28747, 393, 786, 4585, 290, 831, 296, 3482, 28723, 13, 20536, 28802, 28747, 334, 1899, 293, 289, 10717, 2402, 28723, 13, 24001, 28747, 2213, 11234, 289, 21065, 350, 385, 2283, 872, 385, 28723, 13, 13, 6570, 10125, 1037, 521, 28324, 28709, 28747, 13, 13, 3104, 28747, 384, 385, 281, 12697, 28725, 3459, 3958, 6700, 3557, 1452, 4882, 281, 12697, 11545, 955, 318, 2892, 25062, 9584, 28717, 10608, 481, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 3833, 2780, 2567, 955, 264, 20482, 15546, 283, 9927, 28725, 3913, 20594, 337, 25403, 28725, 521, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 2646, 281, 1331, 385, 27947, 340, 955, 2635, 4498, 361, 293, 340, 318, 2892, 25062, 295, 323, 269, 957, 11174, 20423, 17154, 2892, 955, 22544, 955, 981, 2220, 4498, 4807, 16262, 2635, 1908, 293, 15807, 16779, 293, 7445, 15985, 28747, 12012, 28780, 4104, 1869, 5936, 28735, 2892, 25062, 647, 464, 28708, 20482, 647, 464, 450, 318, 2892, 25062, 5807, 464, 28780, 11724, 1869, 5936, 269, 408, 2465, 28708, 340, 710, 2925, 28708, 481, 543, 3217, 512, 17542, 5807, 464, 20536, 1020, 1869, 5936, 23718, 20594, 337, 25403, 5807, 464, 20536, 962, 1869, 5936, 370, 3102, 340, 28705, 28783, 28787, 2545, 2402, 340, 7255, 370, 293, 647, 464, 11215, 4498, 361, 293, 647, 464, 3368, 17154, 2892, 955, 22544, 1421, 28752, 13, 13, 28741, 18701, 28725, 2691, 28708, 543, 19846, 8783, 261, 7458, 28747, 13, 13, 3104, 28747, 627, 1037, 955, 3979, 10167, 5227, 28708, 639, 7153, 12206, 6216, 3756, 28725, 481, 521, 1524, 12639, 28709, 955, 11127, 274, 462, 263, 481, 7021, 481, 22918, 28725, 639, 420, 598, 20122, 340, 20482, 3631, 1083, 28728, 831, 296, 1452, 379, 28705, 28781, 28782, 28723, 28774, 28783, 28734, 317, 22751, 29000, 370, 934, 3704, 7469, 639, 5088, 28709, 3142, 3581, 340, 1515, 284, 4812, 1065, 28723, 15985, 28747, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos. Output:</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
    "\n",
    "WHO: Sujetos o entidades involucradas.\n",
    "WHAT: Hechos u objetos mencionados.\n",
    "WHEN: Detalles relacionados con el tiempo.\n",
    "WHERE: Lugares mencionados.\n",
    "WHY: Causas o razones.\n",
    "HOW: Maneras o métodos descritos.\n",
    "\n",
    "Abajo es un ejemplo:\n",
    "\n",
    "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output:{'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
    "\n",
    "Ahora, completa la siguiente tarea:\n",
    "\n",
    "Input: Pero no pasará nada, el PSOE no exigirá la dimisión de este machista, porque para eso es de los suyos, de la izquierda rabiosa de este país que se dedica a dar lecciones de feminismo y a la mínima de cambio insultan a las mujeres por su forma de vestir. Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'WHO': ['el PSOE',\n",
    "  'de este machista',\n",
    "  'de la izquierda rabiosa',\n",
    "  'a las mujeres'],\n",
    " 'WHAT': ['la dimisión', 'a dar lecciones de feminismo'],\n",
    " 'WHY': ['porque para eso es de los suyos', 'por su forma de vestir'],\n",
    " 'WHERE': ['de este país']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
    "\n",
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`mistral-journal-finetune`) as your final model in step 6 below.\n",
    "\n",
    "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/FLARE-Challenge/Mistral-models/wandb/run-20240513_154539-eqo1oyvt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiayunliu2000/journal-finetune/runs/eqo1oyvt' target=\"_blank\">mistral-7b-flare-finetune-JSON-format-full-2024-05-13-15-45</a></strong> to <a href='https://wandb.ai/jiayunliu2000/journal-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiayunliu2000/journal-finetune' target=\"_blank\">https://wandb.ai/jiayunliu2000/journal-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiayunliu2000/journal-finetune/runs/eqo1oyvt' target=\"_blank\">https://wandb.ai/jiayunliu2000/journal-finetune/runs/eqo1oyvt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2379' max='2379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2379/2379 55:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.232660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.200718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.191356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.181889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.171722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.166339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.252400</td>\n",
       "      <td>0.159008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.225700</td>\n",
       "      <td>0.150451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.138411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.132286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.126079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.120146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.115981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.107643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.104134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.092851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.087654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.083117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.078241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.073329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.070947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.067931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.114300</td>\n",
       "      <td>0.067815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2379, training_loss=0.19631034238541914, metrics={'train_runtime': 3335.6921, 'train_samples_per_second': 1.425, 'train_steps_per_second': 0.713, 'total_flos': 1.43705349623808e+17, 'train_loss': 0.19631034238541914, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"flare-finetune-JSON-format-full\"\n",
    "base_model_name = \"mistral-7b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=100,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model_path = \"mistral-7b-flare-finetune-JSON-format-full\"\n",
    "ft_model = PeftModel.from_pretrained(base_model, f\"{model_path}/checkpoint-2300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "Set stopping criteria to stop the model generating garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"}\", \"'}}\", \"'}\\n\", \"}}\\n\", \"'}\\n\\n\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
    "\n",
    "WHO: Sujetos o entidades involucradas.\n",
    "WHAT: Hechos u objetos mencionados.\n",
    "WHEN: Detalles relacionados con el tiempo.\n",
    "WHERE: Lugares mencionados.\n",
    "WHY: Causas o razones.\n",
    "HOW: Maneras o métodos descritos.\n",
    "\n",
    "Abajo es un ejemplo:\n",
    "\n",
    "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
    "\n",
    "Ahora, completa la siguiente tarea:\n",
    "\n",
    "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output:\n",
    "\"\"\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'WHO': ['Sánchez', 'a España', 'de Sánchez'],\n",
    " 'WHERE': ['en rueda de prensa en la Moncloa'],\n",
    " 'WHEN': ['entre abril y septiembre'],\n",
    " 'WHAT': ['un total de 87 millones de vacunas',\n",
    "  'las mentiras',\n",
    "  'ese refrán que dice']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
    "\n",
    "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
    "\n",
    "That output is quite private but I wanted you to see an example run, so I tweaked the `eval_prompt` so that it explicitly wouldn't say anything too sensitive, haha.\n",
    "\n",
    "I hope you enjoyed this tutorial on fine-tuning Mistral on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/RN2a436M73).\n",
    "\n",
    "🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙 🤙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### 7. FLARE Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 TASK\n",
    "print(tokenized_train_dataset.shape)\n",
    "print(tokenized_val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: Esto se refleja en la ocupación de unidades de cuidados intensivos (UCI), lo que se agrava con la llegada de pacientes remitidos de otras ciudades de la región. Output: \n"
     ]
    }
   ],
   "source": [
    "model_input = f\"{tokenized_val_dataset[0]['input']} Output: \"\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    model_input = eval_tokenizer(model_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu tarea consiste en analizar el texto proporcionado y extraer fragmentos significativos, transcribiéndolos exactamente como aparecen. Asigna a cada fragmento la etiqueta correspondiente basada en la información que representa. Presenta los resultados en formato JSON. Las categorías de etiquetado son:\n",
      "\n",
      "WHO: Sujetos o entidades involucradas.\n",
      "WHAT: Hechos u objetos mencionados.\n",
      "WHEN: Detalles relacionados con el tiempo.\n",
      "WHERE: Lugares mencionados.\n",
      "WHY: Causas o razones.\n",
      "HOW: Maneras o métodos descritos.\n",
      "\n",
      "Abajo es un ejemplo:\n",
      "\n",
      "Input: Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”. Output: {'WHO': ['Sánchez', 'a España', 'de Sánchez'], 'WHERE': ['en rueda de prensa en la Moncloa'], 'WHEN': ['entre abril y septiembre'], 'WHAT': ['un total de 87 millones de vacunas', 'las mentiras', 'ese refrán que dice']}\n",
      "\n",
      "Ahora, completa la siguiente tarea:\n",
      "\n",
      "Input: Esto se refleja en la ocupación de unidades de cuidados intensivos (UCI), lo que se agrava con la llegada de pacientes remitidos de otras ciudades de la región. Output:  {'HOW': None, 'WHAT': ['en la ocupación de unidades de cuidados intensivos (UCI)', 'con la llegada de pacientes remitidos'], 'WHEN': None, 'WHERE': ['de otras ciudades de la región'], 'WHO': None, 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HOW': None, 'WHAT': ['en la ocupación de unidades de cuidados intensivos (UCI)', 'con la llegada de pacientes remitidos'], 'WHEN': None, 'WHERE': ['de otras ciudades de la región'], 'WHO': None, 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the generated text transform to FLARE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent generating text: 2999.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "ft_model.eval()\n",
    "res = []\n",
    "\n",
    "start_time = time.time()  # Start timing before the loop\n",
    "\n",
    "for x in tokenized_val_dataset:\n",
    "    prompt = f\"{x['input']} Output: \"\n",
    "    model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]\n",
    "        input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "        generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "        res.append(generated_text)\n",
    "        \n",
    "end_time = time.time()  # End timing after the loop\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time spent generating text: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(f\"{model_path}/test_results_2300iter.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(res, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"mistral-7b-flare-finetune-JSON-format-full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "# Load results\n",
    "with open(f\"{model_path}/test_results_2300iter.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    res = json.load(file)\n",
    "\n",
    "# Load the data\n",
    "task1_train = pd.read_json('../Flares-dataset/5w1h_subtarea_1_train.json', lines=True)\n",
    "task1_test = pd.read_json('../Flares-dataset/5w1h_subtarea_1_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index_format(data_dict, original_text):\n",
    "    tags_list = []\n",
    "\n",
    "    for label, fragments in data_dict.items():\n",
    "        if fragments is not None:\n",
    "            for fragment in fragments:\n",
    "                start_index = original_text.find(fragment)\n",
    "                if start_index != -1:\n",
    "                    end_index = start_index + len(fragment)\n",
    "                    tags_list.append({\n",
    "                        'Tag_Start': start_index,\n",
    "                        'Tag_End': end_index,\n",
    "                        '5W1H_Label': label,\n",
    "                        'Tag_Text': fragment\n",
    "                    })\n",
    "    \n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Esto se refleja en la ocupación de unidades de cuidados intensivos (UCI), lo que se agrava con la llegada de pacientes remitidos de otras ciudades de la región.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_test['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dos días, exactamente han pasado dos días desde que Sánchez compareciera en rueda de prensa en la Moncloa afirmando que a España llegarían, entre abril y septiembre, un total de 87 millones de vacunas para darnos cuenta de que las mentiras de Sánchez hacen bueno ese refrán que dice que “la mentira tiene las patas muy cortas”.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_train['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'HOW': None, 'WHAT': ['en la ocupación de unidades de cuidados intensivos (UCI)', 'con la llegada de pacientes remitidos'], 'WHEN': None, 'WHERE': ['de otras ciudades de la región'], 'WHO': None, 'WHY': None}\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Initialize an empty list to store the formatted lists\n",
    "all_formatted_lists = []\n",
    "\n",
    "# Assuming 'convert_to_index_format' is a function and 'task1_test' is a dataset with a column 'Text'\n",
    "for i in range(len(task1_test['Text'])):\n",
    "    try:\n",
    "        # Convert the current item and get the formatted list\n",
    "        formatted_list = convert_to_index_format(ast.literal_eval(res[i]), task1_test['Text'][i])\n",
    "\n",
    "        # Append the formatted list to the collection of all formatted lists\n",
    "        all_formatted_lists.append(formatted_list)\n",
    "        \n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        # If an error occurs, simply skip this item and continue with the next\n",
    "        all_formatted_lists.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting each sublist by 'Tag_Start' key\n",
    "sorted_all_formatted_lists = [sorted(sub_list, key=lambda x: x['Tag_Start']) for sub_list in all_formatted_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Id', 'output', 'input', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1268\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Generated tags\n",
      "[{'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 72,\n",
      "  'Tag_Start': 16,\n",
      "  'Tag_Text': 'en la ocupación de unidades de cuidados intensivos (UCI)'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 128,\n",
      "  'Tag_Start': 91,\n",
      "  'Tag_Text': 'con la llegada de pacientes remitidos'},\n",
      " {'5W1H_Label': 'WHERE',\n",
      "  'Tag_End': 159,\n",
      "  'Tag_Start': 129,\n",
      "  'Tag_Text': 'de otras ciudades de la región'}]\n",
      "####Ground Truh tags\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pp\u001b[38;5;241m.\u001b[39mpprint(sorted_all_formatted_lists[i])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m####Ground Truh tags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m pp\u001b[38;5;241m.\u001b[39mpprint(\u001b[43mtokenized_val_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1, width=80, depth=None, compact=False)\n",
    "for i in range(0,2):\n",
    "    print(f\"####Generated tags\")\n",
    "    pp.pprint(sorted_all_formatted_lists[i])\n",
    "    print(f\"####Ground Truh tags\")\n",
    "    pp.pprint(tokenized_val_dataset[i]['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract both 'tags' and 'Id' from each entry and store them in a list of dictionaries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tokenized_val_dataset]\n",
      "Cell \u001b[0;32mIn [58], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract both 'tags' and 'Id' from each entry and store them in a list of dictionaries\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m: entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m tokenized_val_dataset]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tags'"
     ]
    }
   ],
   "source": [
    "# Extract both 'tags' and 'Id' from each entry and store them in a list of dictionaries\n",
    "ground_truth = [{'Id': entry['Id'], 'Tags': f\"{entry['tags']}\"} for entry in tokenized_val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mground_truth\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Correct matches** are reported when a text in the predicted file matches exactly with a corresponding text span in the gold file for start and end index, and also the 5W1H label. Only one correct match per entry in the gold file can be matched. \n",
    "\n",
    "**Incorrect matches** are reported when the start and end index match, but not the type.\n",
    "\n",
    "**Partial matches** are reported when two intervals [start, end] have a non-empty intersection, such as the case of “scientists” and “scientists specialised in biophysics” in the previous example (and matching the 5W1H label). Notice that a partial phrase will only be matched against a single correct phrase. For example, “researchers from the GPLSI Group” could be a partial match for both “researchers” and “the GPLSI Group”, but it is only counted once as a partial match with the word “researchers”. The words “the GPLSI Group” are counted then as Missing. This aims to discourage a few large text spans that cover most of the document from getting a very high score. \n",
    "\n",
    "**Missing matches** are those that appear in the gold file but not in the predicted file. \n",
    "\n",
    "**Spurious matches** are those that appear in the predicted file but not in the gold file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVirginia Evaluation code\\n# Generated using ChatGPT\\n\\ndef evaluate_spans(predicted_spans, gold_spans):\\n    # Initialize counters for different types of matches\\n    total_correct_matches = 0\\n    total_incorrect_matches = 0\\n    total_partial_matches = 0\\n    total_missing_matches = 0\\n    total_spurious_matches = 0\\n\\n    # Iterate over each pair of predicted and gold documents\\n    for predicted_spans, gold_spans in zip(predicted_spans, gold_spans):\\n        correct_matches = 0\\n        incorrect_matches = 0\\n        partial_matches = 0\\n        missing_matches = len(gold_spans)  # Initialize with total gold spans for this document\\n        spurious_matches = 0\\n\\n        # Iterate through each gold span in the current document\\n        for gold_span in gold_spans:\\n            found_match = False  # Flag to track if a match is found for the current gold span\\n\\n            # Iterate through each predicted span in the current document\\n            for predicted_span in predicted_spans:\\n                # Check if start and end indices and labels match exactly\\n                if (gold_span[\\'Tag_Start\\'] == predicted_span[\\'Tag_Start\\'] and\\n                    gold_span[\\'Tag_End\\'] == predicted_span[\\'Tag_End\\'] and\\n                    gold_span[\\'5W1H_Label\\'] == predicted_span[\\'5W1H_Label\\']):\\n                    correct_matches += 1\\n                    found_match = True\\n                    break\\n\\n                # Check if start and end indices match but labels don\\'t\\n                elif (gold_span[\\'Tag_Start\\'] == predicted_span[\\'Tag_Start\\'] and\\n                      gold_span[\\'Tag_End\\'] == predicted_span[\\'Tag_End\\']):\\n                    incorrect_matches += 1\\n                    found_match = True\\n                    break\\n\\n                # Check for partial matches\\n                elif (gold_span[\\'Tag_Start\\'] <= predicted_span[\\'Tag_End\\'] and\\n                      gold_span[\\'Tag_End\\'] >= predicted_span[\\'Tag_Start\\']):\\n                    partial_matches += 1\\n                    found_match = True\\n                    break\\n\\n            # If no match is found, decrement the missing match counter\\n            if found_match:\\n                missing_matches -= 1\\n\\n        # Calculate spurious matches for the current document\\n        spurious_matches = len(predicted_spans) - (correct_matches + incorrect_matches + partial_matches)\\n\\n        # Accumulate document-level results into total results\\n        total_correct_matches += correct_matches\\n        total_incorrect_matches += incorrect_matches\\n        total_partial_matches += partial_matches\\n        total_missing_matches += missing_matches\\n        total_spurious_matches += spurious_matches\\n\\n    return total_correct_matches, total_incorrect_matches, total_partial_matches, total_missing_matches, total_spurious_matches\\n\\n\\n# Example usage:\\ngold_spans = [{\"Tag_Start\":5,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"los investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHAT\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":102,\"Tag_End\":146,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\\npredicted_spans = [{\"Tag_Start\":9,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHERE\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":24,\"Tag_End\":38,\"5W1H_Label\":\"HOW\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\\n\\n# Call the function with gold and predicted spans\\ncorrect, incorrect, partial, missing, spurious = evaluate_spans(sorted_all_formatted_lists, ground_truth)\\n\\n# Print the evaluation metrics\\nprint(\"Correct Matches:\", correct)\\nprint(\"Incorrect Matches:\", incorrect)\\nprint(\"Partial Matches:\", partial)\\nprint(\"Missing Matches:\", missing)\\nprint(\"Spurious Matches:\", spurious)\\n\\nrecall = (correct + (0.5 * partial)) / (correct + incorrect + partial + missing)\\n\\nprecision = (correct + (0.5 * partial)) / (correct + incorrect + partial + spurious)\\n\\nf1 = 2 * (precision * recall) / (precision + recall)\\n\\nprint(f\"Recall: {recall}, Precision: {precision}, F1: {f1}\")\\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Virginia Evaluation code\n",
    "# Generated using ChatGPT\n",
    "\n",
    "def evaluate_spans(predicted_spans, gold_spans):\n",
    "    # Initialize counters for different types of matches\n",
    "    total_correct_matches = 0\n",
    "    total_incorrect_matches = 0\n",
    "    total_partial_matches = 0\n",
    "    total_missing_matches = 0\n",
    "    total_spurious_matches = 0\n",
    "\n",
    "    # Iterate over each pair of predicted and gold documents\n",
    "    for predicted_spans, gold_spans in zip(predicted_spans, gold_spans):\n",
    "        correct_matches = 0\n",
    "        incorrect_matches = 0\n",
    "        partial_matches = 0\n",
    "        missing_matches = len(gold_spans)  # Initialize with total gold spans for this document\n",
    "        spurious_matches = 0\n",
    "\n",
    "        # Iterate through each gold span in the current document\n",
    "        for gold_span in gold_spans:\n",
    "            found_match = False  # Flag to track if a match is found for the current gold span\n",
    "\n",
    "            # Iterate through each predicted span in the current document\n",
    "            for predicted_span in predicted_spans:\n",
    "                # Check if start and end indices and labels match exactly\n",
    "                if (gold_span['Tag_Start'] == predicted_span['Tag_Start'] and\n",
    "                    gold_span['Tag_End'] == predicted_span['Tag_End'] and\n",
    "                    gold_span['5W1H_Label'] == predicted_span['5W1H_Label']):\n",
    "                    correct_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "                # Check if start and end indices match but labels don't\n",
    "                elif (gold_span['Tag_Start'] == predicted_span['Tag_Start'] and\n",
    "                      gold_span['Tag_End'] == predicted_span['Tag_End']):\n",
    "                    incorrect_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "                # Check for partial matches\n",
    "                elif (gold_span['Tag_Start'] <= predicted_span['Tag_End'] and\n",
    "                      gold_span['Tag_End'] >= predicted_span['Tag_Start']):\n",
    "                    partial_matches += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "\n",
    "            # If no match is found, decrement the missing match counter\n",
    "            if found_match:\n",
    "                missing_matches -= 1\n",
    "\n",
    "        # Calculate spurious matches for the current document\n",
    "        spurious_matches = len(predicted_spans) - (correct_matches + incorrect_matches + partial_matches)\n",
    "\n",
    "        # Accumulate document-level results into total results\n",
    "        total_correct_matches += correct_matches\n",
    "        total_incorrect_matches += incorrect_matches\n",
    "        total_partial_matches += partial_matches\n",
    "        total_missing_matches += missing_matches\n",
    "        total_spurious_matches += spurious_matches\n",
    "\n",
    "    return total_correct_matches, total_incorrect_matches, total_partial_matches, total_missing_matches, total_spurious_matches\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "gold_spans = [{\"Tag_Start\":5,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"los investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHAT\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":102,\"Tag_End\":146,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\n",
    "predicted_spans = [{\"Tag_Start\":9,\"Tag_End\":23,\"5W1H_Label\":\"WHO\",\"Reliability_Label\":\"semiconfiable\",\"Tag_Text\":\"investigadores\"},{\"Tag_Start\":39,\"Tag_End\":101,\"5W1H_Label\":\"WHERE\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"evidencia de que la aspirina aumentara el riesgo de hemorragia\"},{\"Tag_Start\":24,\"Tag_End\":38,\"5W1H_Label\":\"HOW\",\"Reliability_Label\":\"confiable\",\"Tag_Text\":\"en los pacientes hospitalizados con COVID-19\"}]\n",
    "\n",
    "# Call the function with gold and predicted spans\n",
    "correct, incorrect, partial, missing, spurious = evaluate_spans(sorted_all_formatted_lists, ground_truth)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Correct Matches:\", correct)\n",
    "print(\"Incorrect Matches:\", incorrect)\n",
    "print(\"Partial Matches:\", partial)\n",
    "print(\"Missing Matches:\", missing)\n",
    "print(\"Spurious Matches:\", spurious)\n",
    "\n",
    "recall = (correct + (0.5 * partial)) / (correct + incorrect + partial + missing)\n",
    "\n",
    "precision = (correct + (0.5 * partial)) / (correct + incorrect + partial + spurious)\n",
    "\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Recall: {recall}, Precision: {precision}, F1: {f1}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the text content\n",
    "#metrics_content = f\"\"\"\n",
    "#Correct Matches: {correct}\n",
    "#Incorrect Matches: {incorrect}\n",
    "#Partial Matches: {partial}\n",
    "#Missing Matches: {missing}\n",
    "#Spurious Matches: {spurious}\n",
    "#Recall: {recall:.4f}\n",
    "#Precision: {precision:.4f}\n",
    "#F1 Score: {f1:.4f}\n",
    "#\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "#with open(f\"{model_path}/evaluation_metrics.txt\", \"w\") as file:\n",
    "#    file.write(metrics_content)\n",
    "\n",
    "#print(\"Metrics have been saved to 'evaluation_metrics.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Tags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using the IDs and the corresponding formatted tag lists\n",
    "tags_df = pd.DataFrame({\n",
    "    'Id': task1_test['Id'],\n",
    "    'Tags': [str(tags) for tags in sorted_all_formatted_lists]  # Convert each list to a string and enclose it in single quotes\n",
    "})\n",
    "\n",
    "print(tags_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "tags_df.to_csv(f\"{model_path}/test_tags_data_2300iter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mground_truth\u001b[49m[\u001b[38;5;241m135\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mground_truth\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth = pd.DataFrame(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now save this DataFrame to a CSV file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mground_truth\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ground_truth.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "ground_truth.to_csv(f\"{model_path}/ground_truth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mground_truth\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTags\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m135\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth' is not defined"
     ]
    }
   ],
   "source": [
    "ground_truth['Tags'][135]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST FLARE CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                               Tags\n",
      "0     72  [{'Tag_Start': 12, 'Tag_End': 35, '5W1H_Label'...\n",
      "1     64  [{'Tag_Start': 0, 'Tag_End': 35, '5W1H_Label':...\n",
      "2     49  [{'Tag_Start': 0, 'Tag_End': 29, '5W1H_Label':...\n",
      "3    130  [{'Tag_Start': 15, 'Tag_End': 40, '5W1H_Label'...\n",
      "4     61  [{'Tag_Start': 0, 'Tag_End': 19, '5W1H_Label':...\n",
      "..   ...                                                ...\n",
      "163   23  [{'Tag_Start': 12, 'Tag_End': 20, '5W1H_Label'...\n",
      "164  129  [{'Tag_Start': 0, 'Tag_End': 38, '5W1H_Label':...\n",
      "165  102  [{'Tag_Start': 17, 'Tag_End': 74, '5W1H_Label'...\n",
      "166    0  [{'Tag_Start': 223, 'Tag_End': 246, '5W1H_Labe...\n",
      "167  104  [{'Tag_Start': 12, 'Tag_End': 83, '5W1H_Label'...\n",
      "\n",
      "[168 rows x 2 columns]\n",
      "[{'Precision': 0.8953488372093024, 'Recall': 0.9083484573502723, 'F1': 0.9018018018018019, 'Accuracy': 0.8570205479452054}]\n"
     ]
    }
   ],
   "source": [
    "!python \"../evaluate_subtask_1.py\" --pathDataGold /mistral-7b-flare-finetune-JSON-format-full/ground_truth.csv --pathDataInfered /mistral-7b-flare-finetune-JSON-format-full/tags_data_2300iter.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLARE CHALLENGE RESULTS ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "task1_train = pd.read_json('../Flares-dataset/5w1h_subtarea_1_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
