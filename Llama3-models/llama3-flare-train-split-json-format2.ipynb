{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC-9m2yv3m18"
   },
   "source": [
    "## Let's begin!\n",
    "Notebook based on brevdev notebook: https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb\n",
    "### 0. Preparing data\n",
    "\n",
    "Before you check out a GPU, prepare your dataset for loading and training.\n",
    "\n",
    "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
    "```\n",
    "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
    "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Instantiate GPU & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6484c41078d64962bba5aa202fd33f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3079af48e00747d2876243935fcf24ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='../data_files/json_format2/5w1h_subtask_1_zero_train_train_json_format2.json', encoding = 'utf-8',split='train')\n",
    "eval_dataset = load_dataset('json', data_files='../data_files/json_format2/5w1h_subtask_1_zero_train_test_json_format2.json', encoding = 'utf-8', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': 345,\n",
       " 'input': '### Instruction:\\nTu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \\nluego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \\nLos resultados deben ser presentados en formato JSON.\\n\\nCategorías de Etiquetado\\nCada fragmento extraído debe clasificarse en una de las siguientes categorías:\\n\\nWHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\\n\\nWHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \\ninvolucrados o mencionados en el texto.\\n\\nWHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\\n\\nWHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\\n\\nWHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\\n\\nHOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \\nse realizan o suceden las cosas.\\n\\n### Input:\\nLa decisión, señala el dictamen, corresponde a la persona afectada, que puede prestar o negar su \\xa0consentimiento.',\n",
       " 'output': {'HOW': None,\n",
       "  'WHAT': ['La decisión', 'su \\xa0consentimiento'],\n",
       "  'WHEN': None,\n",
       "  'WHERE': None,\n",
       "  'WHO': ['a la persona afectada'],\n",
       "  'WHY': None},\n",
       " 'tags': [{'5W1H_Label': 'WHAT',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 11,\n",
       "   'Tag_Start': 0,\n",
       "   'Tag_Text': 'La decisión'},\n",
       "  {'5W1H_Label': 'WHO',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 66,\n",
       "   'Tag_Start': 45,\n",
       "   'Tag_Text': 'a la persona afectada'},\n",
       "  {'5W1H_Label': 'WHAT',\n",
       "   'Reliability_Label': 'confiable',\n",
       "   'Tag_End': 112,\n",
       "   'Tag_Start': 94,\n",
       "   'Tag_Text': 'su \\xa0consentimiento'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[306]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### Accelerator\n",
    "\n",
    "Set up the Accelerator ([description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"Flares-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhw8JiOr3m18"
   },
   "source": [
    "### Formatting prompts\n",
    "Then create a `formatting_func` to structure training examples as prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"{example['input']}\\n\\n### Output:\\n{example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sflV0DL2P64_"
   },
   "source": [
    "Here's another common one:\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"{example['input']} Output: {example['output']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Llama3 - meta-llama/Meta-Llama-3-8B - using 4-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JupyterLab v3.4.6\n",
      "/opt/conda/share/jupyter/labextensions\n",
      "        jupyterlab_pygments v0.2.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_pygments)\n",
      "        jupyter-matplotlib v0.11.2 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m\n",
      "        @jupyter-widgets/jupyterlab-manager v5.0.3 \u001b[32menabled\u001b[0m \u001b[32mOK\u001b[0m (python, jupyterlab_widgets)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!jupyter labextension list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "45524c98039a46d5b7745ad7cb638d2f"
     ]
    },
    "id": "E0Nl5mWL0k2T",
    "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bfd552e1bb466ab8afe262ee009c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "haSUDD9HyRgf",
    "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S3iLAwLh3m19"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fb5d8b24f04d9097e64b54ff083656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb5eb0462c0404086bd0985738127b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ewk27p3m19"
   },
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMlw8h743m19"
   },
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "acINaViR3m19"
   },
   "outputs": [],
   "source": [
    "max_length = 700 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "518d4f0b89bf4d57bf00d4c6d6e59eb5"
     ]
    },
    "id": "lTk-aTog3m19",
    "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97ee1b8cc974472b61c74bbb255b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579a28af2ccd43a09bd82c2d0fce156a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OKHhvxK83m19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000, 14711, 30151, 512, 54071, 91007, 1560, 3260, 11403, 658, 33125, 80943, 2172, 3429, 3608, 24123, 379, 5066, 261, 12569, 437, 4698, 64545, 11, 720, 75, 27779, 61179, 277, 264, 19394, 12569, 78, 5203, 62044, 1955, 1744, 5126, 4749, 1208, 14818, 80252, 409, 1208, 35615, 1744, 81533, 13, 720, 30696, 47822, 74713, 1446, 3118, 5670, 665, 55956, 4823, 382, 34, 7747, 25318, 409, 19421, 5118, 295, 2172, 198, 34, 2649, 12569, 78, 5066, 2483, 3055, 35474, 57220, 1104, 2648, 665, 5203, 409, 5252, 86961, 22824, 25318, 1473, 60960, 320, 66806, 1680, 1369, 5771, 6181, 2537, 568, 46348, 11, 75148, 11, 297, 47009, 41718, 390, 846, 998, 52618, 290, 2172, 665, 658, 33125, 382, 78847, 320, 62545, 10610, 1680, 23322, 30588, 264, 2537, 70927, 437, 297, 1218, 13910, 320, 9164, 300, 11, 25286, 12712, 11, 5099, 6266, 1744, 42939, 720, 258, 12821, 1791, 81, 5670, 297, 52618, 290, 5670, 665, 658, 33125, 382, 20484, 965, 320, 45919, 48680, 78, 1680, 26041, 68, 92429, 1744, 69694, 276, 97253, 297, 4261, 437, 409, 29842, 52618, 290, 5670, 665, 658, 33125, 382, 27611, 320, 35, 1832, 43441, 1680, 8949, 17528, 2537, 93924, 11, 16948, 80945, 3980, 83581, 17038, 297, 13510, 55803, 52618, 290, 11354, 665, 658, 33125, 382, 20484, 56, 320, 29197, 43388, 1680, 23322, 30588, 5252, 25540, 300, 11, 24788, 3233, 297, 12521, 12712, 56369, 11354, 665, 658, 33125, 382, 61297, 320, 96997, 1680, 61885, 5252, 893, 9431, 11, 36252, 16790, 297, 7248, 46062, 1744, 658, 33125, 3474, 21575, 15482, 55996, 720, 325, 34860, 276, 297, 924, 2041, 268, 5252, 56623, 382, 14711, 5688, 512, 2118, 17812, 23321, 36897, 58548, 48591, 7617, 300, 11, 58892, 1744, 68681, 68160, 665, 653, 31221, 23321, 33397, 11, 687, 34229, 8862, 379, 39974, 4988, 18922, 531, 39961, 3429, 1208, 687, 44731, 409, 2537, 46418, 10006, 36550, 297, 63190, 300, 1744, 15833, 3118, 4988, 658, 29452, 9520, 93224, 4168, 1832, 658, 3468, 15931, 409, 8375, 664, 382, 14711, 9442, 512, 13922, 61297, 1232, 2290, 11, 364, 60960, 1232, 2290, 11, 364, 20484, 965, 1232, 2290, 11, 364, 27611, 1232, 2290, 11, 364, 78847, 1232, 2570, 301, 3468, 15931, 409, 8375, 664, 4181, 364, 20484, 56, 1232, 2290, 92]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 18:50:14.030201: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "“Es muy importante seguir estas normas, dado que nosotros estamos en un momento muy especial, conteniendo y trabajando fuertemente para la contención de los diferentes linajes o cepas que está presentando el mundo”, concluyó el Ministro de Salud.\n",
      "\n",
      "### Output:\n",
      "{'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['el Ministro de Salud'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset[2]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Id', 'input', 'output', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1268\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['input'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000, 14711, 30151, 512, 54071, 91007, 1560, 3260, 11403, 658, 33125, 80943, 2172, 3429, 3608, 24123, 379, 5066, 261, 12569, 437, 4698, 64545, 11, 720, 75, 27779, 61179, 277, 264, 19394, 12569, 78, 5203, 62044, 1955, 1744, 5126, 4749, 1208, 14818, 80252, 409, 1208, 35615, 1744, 81533, 13, 720, 30696, 47822, 74713, 1446, 3118, 5670, 665, 55956, 4823, 382, 34, 7747, 25318, 409, 19421, 5118, 295, 2172, 198, 34, 2649, 12569, 78, 5066, 2483, 3055, 35474, 57220, 1104, 2648, 665, 5203, 409, 5252, 86961, 22824, 25318, 1473, 60960, 320, 66806, 1680, 1369, 5771, 6181, 2537, 568, 46348, 11, 75148, 11, 297, 47009, 41718, 390, 846, 998, 52618, 290, 2172, 665, 658, 33125, 382, 78847, 320, 62545, 10610, 1680, 23322, 30588, 264, 2537, 70927, 437, 297, 1218, 13910, 320, 9164, 300, 11, 25286, 12712, 11, 5099, 6266, 1744, 42939, 720, 258, 12821, 1791, 81, 5670, 297, 52618, 290, 5670, 665, 658, 33125, 382, 20484, 965, 320, 45919, 48680, 78, 1680, 26041, 68, 92429, 1744, 69694, 276, 97253, 297, 4261, 437, 409, 29842, 52618, 290, 5670, 665, 658, 33125, 382, 27611, 320, 35, 1832, 43441, 1680, 8949, 17528, 2537, 93924, 11, 16948, 80945, 3980, 83581, 17038, 297, 13510, 55803, 52618, 290, 11354, 665, 658, 33125, 382, 20484, 56, 320, 29197, 43388, 1680, 23322, 30588, 5252, 25540, 300, 11, 24788, 3233, 297, 12521, 12712, 56369, 11354, 665, 658, 33125, 382, 61297, 320, 96997, 1680, 61885, 5252, 893, 9431, 11, 36252, 16790, 297, 7248, 46062, 1744, 658, 33125, 3474, 21575, 15482, 55996, 720, 325, 34860, 276, 297, 924, 2041, 268, 5252, 56623, 382, 14711, 5688, 512, 56, 1560, 1744, 63115, 6179, 64, 658, 7528, 10619, 7923, 3370, 11, 665, 653, 80689, 1744, 60045, 54238, 665, 10566, 665, 27634, 11, 658, 85172, 409, 57608, 6520, 1207, 85, 93873, 2172, 390, 220, 1774, 13, 19068, 33588, 4194, 359, 80036, 15482, 658, 5536, 78, 1877, 9265, 409, 2537, 9115, 897, 437, 382, 14711, 9442, 512, 13922, 61297, 1232, 2570, 444, 220, 1774, 13, 19068, 33588, 4181, 364, 60960, 1232, 2570, 359, 80036, 15482, 658, 5536, 78, 1877, 9265, 409, 2537, 9115, 897, 437, 4181, 364, 20484, 965, 1232, 2290, 11, 364, 27611, 1232, 2570, 268, 653, 80689, 1744, 60045, 54238, 665, 10566, 665, 27634, 4181, 364, 78847, 1232, 2570, 301, 7528, 10619, 7923, 3370, 518, 364, 301, 85172, 409, 57608, 4181, 364, 20484, 56, 1232, 2290, 92]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
      "\n",
      "### Output:\n",
      "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3R4enP3m19"
   },
   "source": [
    "### How does the base model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxbl4ACsyRgi"
   },
   "source": [
    "Optionally, you can check how Mistral does on one of your data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"### Instruction:\n",
    "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
    "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
    "Los resultados deben ser presentados en formato JSON.\n",
    "\n",
    "Categorías de Etiquetado\n",
    "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
    "\n",
    "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
    "\n",
    "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
    "involucrados o mencionados en el texto.\n",
    "\n",
    "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
    "\n",
    "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
    "\n",
    "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
    "\n",
    "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
    "se realizan o suceden las cosas.\n",
    "\n",
    "### Input:\n",
    "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
    "\n",
    "### Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NidIuFXMyRgi",
    "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
      "\n",
      "### Output:\n",
      "[\n",
      "    {\n",
      "        \"text\": \"Y es que\",\n",
      "        \"category\": \"what\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"según informa el digital OK Diario\",\n",
      "        \"category\": \"who\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"en un artículo que puedes leer en este enlace\",\n",
      "        \"category\": \"where\"\n",
      "    },\n",
      "    {\n",
      "        \"text\": \"el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\",\n",
      "        \"category\": \"why\"\n",
      "    }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Init an eval tokenizer that doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=512, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOW': ['con 45.980 euros'],\n",
       " 'WHAT': ['un estudio sobre el impacto sexista de los piropos'],\n",
       " 'WHEN': None,\n",
       " 'WHERE': ['en un artículo que puedes leer en este enlace'],\n",
       " 'WHO': ['el digital OK Diario', 'el Gobierno de España'],\n",
       " 'WHY': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "Observe how the model does out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Ybeyl20n3dYH",
    "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 88121344 || all params: 4628721664 || trainable%: 1.9037944036550305\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=128256, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired.\n",
    "\n",
    "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir.\n",
    "\n",
    "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "yxSbpKQSLY6B"
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "jq0nX33BmfaC",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/FLARE-Challenge/Llama3-models/wandb/run-20240510_185213-dtxnuk1p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiayunliu2000/Flares-finetune/runs/dtxnuk1p' target=\"_blank\">llama3-8b-flare-finetune-train-split-JSON-Template2-2024-05-10-18-52</a></strong> to <a href='https://wandb.ai/jiayunliu2000/Flares-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiayunliu2000/Flares-finetune' target=\"_blank\">https://wandb.ai/jiayunliu2000/Flares-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiayunliu2000/Flares-finetune/runs/dtxnuk1p' target=\"_blank\">https://wandb.ai/jiayunliu2000/Flares-finetune/runs/dtxnuk1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1902' max='1902' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1902/1902 56:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.356392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.325500</td>\n",
       "      <td>0.321849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.316219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.311745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.310292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.307148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.306958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.309475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.308540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.305588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.307130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.305305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.317788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.199500</td>\n",
       "      <td>0.320983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.323647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.323384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.195100</td>\n",
       "      <td>0.324253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.326114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.325426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1902, training_loss=0.28528655208373044, metrics={'train_runtime': 3399.8294, 'train_samples_per_second': 1.119, 'train_steps_per_second': 0.559, 'total_flos': 1.213125775884288e+17, 'train_loss': 0.28528655208373044, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"flare-finetune-train-split-JSON-Template2\"\n",
    "base_model_name = \"llama3-8b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=100,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9rRmDCeQiTJ"
   },
   "source": [
    "I cleared the output of the cell above because I stopped the training early, and it produced a long, ugly error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 6. Drum Roll... Try the Trained Model!\n",
    "\n",
    "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
    "\n",
    "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "fb8230fb86884aa6be318e2d03a88af2"
     ]
    },
    "id": "SKSnF016yRgp",
    "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6b9bf7f24149c2815b988db00a493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model_path = \"llama3-8b-flare-finetune-train-split-JSON-Template2\"\n",
    "ft_model = PeftModel.from_pretrained(base_model, f\"{model_path}/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "Set stopping criteria to stop the model generating garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1]\n",
    "        for stop in self.stops:\n",
    "            if tokenizer.decode(stop) == tokenizer.decode(last_token):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"}\", \"'}}\", \"'}\\n\", \"}}\\n\", \"'}\\n\\n\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt', add_special_tokens=False)['input_ids'].squeeze() for stop_word in stop_words]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX39ibolyRgp"
   },
   "source": [
    "and run your inference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUehsaVNyRgp"
   },
   "source": [
    "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "lMkVNEUvyRgp",
    "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"### Instruction:\n",
    "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
    "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
    "Los resultados deben ser presentados en formato JSON.\n",
    "\n",
    "Categorías de Etiquetado\n",
    "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
    "\n",
    "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
    "\n",
    "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
    "involucrados o mencionados en el texto.\n",
    "\n",
    "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
    "\n",
    "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
    "\n",
    "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
    "\n",
    "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
    "se realizan o suceden las cosas.\n",
    "\n",
    "### Input:\n",
    "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
    "\n",
    "### Output:\"\"\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros un estudio sobre el impacto sexista de los piropos.\n",
      "\n",
      "### Output: \n",
      "{'HOW': None, 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo', 'en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "{'HOW': None, 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo', 'en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOW': ['con 45.980 euros'],\n",
       " 'WHAT': ['un estudio sobre el impacto sexista de los piropos'],\n",
       " 'WHEN': None,\n",
       " 'WHERE': ['en un artículo que puedes leer en este enlace'],\n",
       " 'WHO': ['el digital OK Diario', 'el Gobierno de España'],\n",
       " 'WHY': None}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'HOW': ['con 45.980 euros'], 'WHAT': ['un estudio sobre el impacto sexista de los piropos'], 'WHEN': None, 'WHERE': ['en un artículo que puedes leer en este enlace'], 'WHO': ['el digital OK Diario', 'el Gobierno de España'], 'WHY': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "### 7. FLARE Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1268, 7)\n",
      "(317, 7)\n"
     ]
    }
   ],
   "source": [
    "# 1 TASK\n",
    "print(tokenized_train_dataset.shape)\n",
    "print(tokenized_val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "\"Es una buena noticia\", ha reivindicado la subdirectora general de Promoción de la Salud, que ha reiterado que desde el departamento catalán consideran que no tendría que haber un límite de edad vinculado a esta vacuna.\n",
      "\n",
      "### Output:\n"
     ]
    }
   ],
   "source": [
    "model_input = f\"{tokenized_val_dataset[0]['input']}\\n\\n### Output:\"\n",
    "print(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    model_input = eval_tokenizer(model_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Tu tarea es analizar el texto proporcionado para identificar y extraer fragmentos significativos, \n",
      "luego asignar a cada fragmento una etiqueta que describa la naturaleza de la información que contiene. \n",
      "Los resultados deben ser presentados en formato JSON.\n",
      "\n",
      "Categorías de Etiquetado\n",
      "Cada fragmento extraído debe clasificarse en una de las siguientes categorías:\n",
      "\n",
      "WHAT (Qué): Señala los hechos, objetos, o cualquier elemento concreto mencionado en el texto.\n",
      "\n",
      "WHO (Quién): Identifica a los sujetos o entidades (personas, organizaciones, etc.) que están \n",
      "involucrados o mencionados en el texto.\n",
      "\n",
      "WHEN (Cuándo): Extrae detalles que especifican momentos o periodos de tiempo mencionados en el texto.\n",
      "\n",
      "WHERE (Dónde): Localiza los lugares, espacios geográficos o direcciones mencionadas en el texto.\n",
      "\n",
      "WHY (Por qué): Identifica las causas, razones o motivaciones explicadas en el texto.\n",
      "\n",
      "HOW (Cómo): Describe las maneras, métodos o procedimientos que el texto detalla sobre cómo \n",
      "se realizan o suceden las cosas.\n",
      "\n",
      "### Input:\n",
      "\"Es una buena noticia\", ha reivindicado la subdirectora general de Promoción de la Salud, que ha reiterado que desde el departamento catalán consideran que no tendría que haber un límite de edad vinculado a esta vacuna.\n",
      "\n",
      "### Output: \n",
      "{'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['la subdirectora general de Promoción de la Salud'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "full_text = eval_tokenizer.decode(generated_token, skip_special_tokens=True)\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "{'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['la subdirectora general de Promoción de la Salud'], 'WHY': None}\n"
     ]
    }
   ],
   "source": [
    "# Decode only the generated text by slicing the tokens from the length of the input\n",
    "input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'5W1H_Label': 'WHO',\n",
       "  'Reliability_Label': 'confiable',\n",
       "  'Tag_End': 88,\n",
       "  'Tag_Start': 40,\n",
       "  'Tag_Text': 'la subdirectora general de Promoción de la Salud'},\n",
       " {'5W1H_Label': 'WHERE',\n",
       "  'Reliability_Label': 'confiable',\n",
       "  'Tag_End': 140,\n",
       "  'Tag_Start': 111,\n",
       "  'Tag_Text': 'desde el departamento catalán'},\n",
       " {'5W1H_Label': 'WHAT',\n",
       "  'Reliability_Label': 'confiable',\n",
       "  'Tag_End': 218,\n",
       "  'Tag_Start': 152,\n",
       "  'Tag_Text': 'que no tendría que haber un límite de edad vinculado a esta vacuna'}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val_dataset[0]['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the generated text transform to FLARE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent generating text: 3043.57 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "ft_model.eval()\n",
    "res = []\n",
    "\n",
    "start_time = time.time()  # Start timing before the loop\n",
    "\n",
    "for x in tokenized_val_dataset:\n",
    "    prompt = f\"{x['input']} Output: \"\n",
    "    model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        generated_token = ft_model.generate(**model_input, max_new_tokens=512, stopping_criteria=stopping_criteria)[0]\n",
    "        input_length = model_input['input_ids'].size(1)  # Number of tokens in the input\n",
    "        generated_text = eval_tokenizer.decode(generated_token[input_length:], skip_special_tokens=True)\n",
    "        res.append(generated_text)\n",
    "        \n",
    "end_time = time.time()  # End timing after the loop\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total time spent generating text: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open(f\"{model_path}/results_1200iter.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(res, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"llama3-8b-flare-finetune-train-split-JSON-Template2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load results\n",
    "with open(f\"{model_path}/results_1900iter.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    res = json.load(file)\n",
    "\n",
    "# Load the data\n",
    "task1_train = pd.read_json('../Flares-dataset/5w1h_subtarea_1_train_train.json', lines=True)\n",
    "task1_test = pd.read_json('../Flares-dataset/5w1h_subtarea_1_train_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_index_format(data_dict, original_text):\n",
    "    tags_list = []\n",
    "\n",
    "    for label, fragments in data_dict.items():\n",
    "        if fragments is not None:\n",
    "            for fragment in fragments:\n",
    "                start_index = original_text.find(fragment)\n",
    "                if start_index != -1:\n",
    "                    end_index = start_index + len(fragment)\n",
    "                    tags_list.append({\n",
    "                        'Tag_Start': start_index,\n",
    "                        'Tag_End': end_index,\n",
    "                        '5W1H_Label': label,\n",
    "                        'Tag_Text': fragment\n",
    "                    })\n",
    "    \n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El experto ha acompañado el mensaje con una fotografía de uno de los casos observados, en el que se ve una lengua manchada o descolorida.\\xa0'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_test['Text'][250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y es que según informa el digital OK Diario, en un artículo que puedes leer en este enlace, el Gobierno de España ha subvencionado con 45.980 euros\\xa0un estudio sobre el impacto sexista de los piropos.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_train['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" {'HOW': None, 'WHAT': None, 'WHEN': None, 'WHERE': None, 'WHO': ['la subdirectora general de Promoción de la Salud'], 'WHY': None}\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Initialize an empty list to store the formatted lists\n",
    "all_formatted_lists = []\n",
    "\n",
    "# Assuming 'convert_to_index_format' is a function and 'task1_test' is a dataset with a column 'Text'\n",
    "for i in range(len(task1_test['Text'])):\n",
    "    try:\n",
    "        # Convert the current item and get the formatted list\n",
    "        formatted_list = convert_to_index_format(ast.literal_eval(res[i]), task1_test['Text'][i])\n",
    "\n",
    "        # Append the formatted list to the collection of all formatted lists\n",
    "        all_formatted_lists.append(formatted_list)\n",
    "        \n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        # If an error occurs, simply skip this item and continue with the next\n",
    "        all_formatted_lists.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting each sublist by 'Tag_Start' key\n",
    "sorted_all_formatted_lists = [sorted(sub_list, key=lambda x: x['Tag_Start']) for sub_list in all_formatted_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Generated tags\n",
      "[{'5W1H_Label': 'WHO',\n",
      "  'Tag_End': 88,\n",
      "  'Tag_Start': 40,\n",
      "  'Tag_Text': 'la subdirectora general de Promoción de la Salud'}]\n",
      "####Ground Truh tags\n",
      "[{'5W1H_Label': 'WHO',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 88,\n",
      "  'Tag_Start': 40,\n",
      "  'Tag_Text': 'la subdirectora general de Promoción de la Salud'},\n",
      " {'5W1H_Label': 'WHERE',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 140,\n",
      "  'Tag_Start': 111,\n",
      "  'Tag_Text': 'desde el departamento catalán'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 218,\n",
      "  'Tag_Start': 152,\n",
      "  'Tag_Text': 'que no tendría que haber un límite de edad vinculado a esta '\n",
      "              'vacuna'}]\n",
      "####Generated tags\n",
      "[{'5W1H_Label': 'WHAT', 'Tag_End': 20, 'Tag_Start': 14, 'Tag_Text': 'dormir'},\n",
      " {'5W1H_Label': 'HOW',\n",
      "  'Tag_End': 58,\n",
      "  'Tag_Start': 21,\n",
      "  'Tag_Text': 'por lo menos entre 6 y 8 horas al día'},\n",
      " {'5W1H_Label': 'WHERE',\n",
      "  'Tag_End': 106,\n",
      "  'Tag_Start': 77,\n",
      "  'Tag_Text': 'en las actividades cotidianas'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 123,\n",
      "  'Tag_Start': 115,\n",
      "  'Tag_Text': 'el humor'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 148,\n",
      "  'Tag_Start': 134,\n",
      "  'Tag_Text': 'el metabolismo'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Tag_End': 177,\n",
      "  'Tag_Start': 164,\n",
      "  'Tag_Text': 'los alimentos'}]\n",
      "####Ground Truh tags\n",
      "[{'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 20,\n",
      "  'Tag_Start': 14,\n",
      "  'Tag_Text': 'dormir'},\n",
      " {'5W1H_Label': 'HOW',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 51,\n",
      "  'Tag_Start': 21,\n",
      "  'Tag_Text': 'por lo menos entre 6 y 8 horas'},\n",
      " {'5W1H_Label': 'WHEN',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 58,\n",
      "  'Tag_Start': 52,\n",
      "  'Tag_Text': 'al día'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 106,\n",
      "  'Tag_Start': 77,\n",
      "  'Tag_Text': 'en las actividades cotidianas'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 123,\n",
      "  'Tag_Start': 115,\n",
      "  'Tag_Text': 'el humor'},\n",
      " {'5W1H_Label': 'WHAT',\n",
      "  'Reliability_Label': 'confiable',\n",
      "  'Tag_End': 148,\n",
      "  'Tag_Start': 134,\n",
      "  'Tag_Text': 'el metabolismo'},\n",
      " {'5W1H_Label': 'WHY',\n",
      "  'Reliability_Label': 'semiconfiable',\n",
      "  'Tag_End': 201,\n",
      "  'Tag_Start': 150,\n",
      "  'Tag_Text': 'de manera que los alimentos se digieren mucho mejor'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1, width=80, depth=None, compact=False)\n",
    "for i in range(0,2):\n",
    "    print(f\"####Generated tags\")\n",
    "    pp.pprint(sorted_all_formatted_lists[i])\n",
    "    print(f\"####Ground Truh tags\")\n",
    "    pp.pprint(tokenized_val_dataset[i]['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract both 'tags' and 'Id' from each entry and store them in a list of dictionaries\n",
    "ground_truth = [{'Id': entry['Id'], 'Tags': f\"{entry['tags']}\"} for entry in tokenized_val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': 1476,\n",
       " 'Tags': \"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\"}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Tags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame using the IDs and the corresponding formatted tag lists\n",
    "tags_df = pd.DataFrame({\n",
    "    'Id': task1_test['Id'],\n",
    "    'Tags': [str(tags) for tags in sorted_all_formatted_lists]  # Convert each list to a string and enclose it in single quotes\n",
    "})\n",
    "\n",
    "print(tags_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "tags_df.to_csv(f\"{model_path}/tags_data_1200iter.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': 1476,\n",
       " 'Tags': \"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\"}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.DataFrame(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this DataFrame to a CSV file\n",
    "ground_truth.to_csv(f\"{model_path}/ground_truth.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'5W1H_Label': 'WHO', 'Reliability_Label': 'confiable', 'Tag_End': 143, 'Tag_Start': 138, 'Tag_Text': 'a\\\\xa0CNN'}]\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth['Tags'][135]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST FLARE CHALLENGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id                                               Tags\n",
      "0    1382  [{'Tag_Start': 40, 'Tag_End': 88, '5W1H_Label'...\n",
      "1     553  [{'Tag_Start': 14, 'Tag_End': 20, '5W1H_Label'...\n",
      "2     567                                                 []\n",
      "3     483  [{'Tag_Start': 15, 'Tag_End': 23, '5W1H_Label'...\n",
      "4     938  [{'Tag_Start': 6, 'Tag_End': 33, '5W1H_Label':...\n",
      "..    ...                                                ...\n",
      "312  1447  [{'Tag_Start': 5, 'Tag_End': 18, '5W1H_Label':...\n",
      "313   583  [{'Tag_Start': 197, 'Tag_End': 237, '5W1H_Labe...\n",
      "314   766                                                 []\n",
      "315  1097  [{'Tag_Start': 72, 'Tag_End': 83, '5W1H_Label'...\n",
      "316   789  [{'Tag_Start': 12, 'Tag_End': 18, '5W1H_Label'...\n",
      "\n",
      "[317 rows x 2 columns]\n",
      "[{'Precision': 0.6121304791029561, 'Recall': 0.42318534178999295, 'F1': 0.5004166666666666, 'Accuracy': 0.3617469879518072}]\n"
     ]
    }
   ],
   "source": [
    "!python \"../evaluate_subtask_1.py\" --pathDataGold /llama3-8b-flare-finetune-train-split-JSON-Template2/ground_truth.csv --pathDataInfered /llama3-8b-flare-finetune-train-split-JSON-Template2/tags_data_1900iter.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
